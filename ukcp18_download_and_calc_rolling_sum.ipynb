{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UKCP 18 rainfall processing\n",
    "\n",
    "- Author: Sam Hardy (modified from Changgui Wang's original Python code)\n",
    "\n",
    "- required input from user (files read from specified path)\n",
    "    - `csv_filename`: `\"YearsMonths_byBinCounts_Rand_OtherYears.csv\"`\n",
    "    - `mask_nc_filename`: `\"UKWC_Cleaned_land-cpm_uk_2.2km.nc\"`\n",
    "    - `year`: e.g. 2024\n",
    "    - `month`: e.g. 12 \n",
    "    - `ensemble_member_id`: e.g. 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and local routines \n",
    "\n",
    "- import local routine: `convert_ll2str.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.config\n",
    "import cftime\n",
    "import timeit\n",
    "import convert_ll2str as c2str\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from base64 import b64encode\n",
    "from datetime import datetime, timezone\n",
    "from getpass import getpass\n",
    "from netCDF4 import Dataset\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "\n",
    "- Calculating 1-h, 3-h and 6-h accumulated precipitation \n",
    "- Calculating data for 13 water companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str):\n",
    "    with open(config_path, 'r') as file: \n",
    "        config = json.load(file)\n",
    "        return config\n",
    "\n",
    "config_path = '/home/jbanorthwest.co.uk/samhardy/ukcp-api-client/config.json' \n",
    "config = load_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2035\n",
    "month = 10\n",
    "ensemble_member_id = 4\n",
    "projection_id = 2 # 2021-2040 time slice\n",
    "var_id = 'pr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINS = {1: [2, 4, 7, 10, 14, 18, 24, 30, 40, 55, 70, 90, 110, 135],\n",
    "        3: [2, 6, 10, 15, 20, 30, 40, 60, 80, 110, 140, 175, 215, 265],\n",
    "        6: [2, 7, 13, 19, 28, 40, 55, 80, 115, 160, 210, 260, 320, 390]}\n",
    "\n",
    "# these values represent the rainfall thresholds for each interval (1-h, 3-h, 6-h) corresponding to different RPs \n",
    "RPS = {1: [19, 24, 32, 36, 42], \n",
    "       3: [29, 35, 44, 49, 57],\n",
    "       6: [35, 42, 53, 59, 67]}\n",
    "\n",
    "rp_years = [5, 10, 30, 50, 100]\n",
    "\n",
    "global PROJECTION_ID\n",
    "\n",
    "NUMBER_OF_WC = 13\n",
    "\n",
    "accum_duration_start = {1: 23, 3: 22, 6: 19}\n",
    "\n",
    "remove_items = ['ensemble_member_id', 'grid_latitude_bnds', 'grid_longitude_bnds',\n",
    "                'time_bnds','rotated_latitude_longitude', 'year', 'yyyymmddhh', 'ensemble_member']\n",
    "squeeze_coords = [\"bnds\", \"ensemble_member\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for retrieving UKCP data using an API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "remote_nc_with_token.py\n",
    "===================\n",
    "\n",
    "Python script for reading a NetCDF file remotely from the CEDA archive. It demonstrates fetching\n",
    "and using a download token to authenticate access to CEDA Archive data, as well as how to load\n",
    "and subset the Dataset from a stream of data (diskless), without having to download the whole file.\n",
    "\n",
    "Pre-requisites:\n",
    "\n",
    " - Python3.x\n",
    " - Python libraries (installed by Pip):\n",
    "\n",
    "```\n",
    "netCDF4\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```\n",
    "$ python remote_nc_with_token.py <url> <var_id>\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "$ URL=https://dap.ceda.ac.uk/badc/ukcp18/data/marine-sim/skew-trend/rcp85/skewSurgeTrend/latest/skewSurgeTrend_marine-sim_rcp85_trend_2007-2099.nc\n",
    "$ VAR_ID=pr\n",
    "\n",
    "$ python remote_nc_with_token.py $URL $VAR_ID\n",
    "```\n",
    "\n",
    "You will be prompted to provide your CEDA username and password the first time the script is run and\n",
    "again if the token cached from a previous attempt has expired.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# URL for the CEDA Token API service\n",
    "TOKEN_URL = \"https://services-beta.ceda.ac.uk/api/token/create/\"\n",
    "# Location on the filesystem to store a cached download token\n",
    "TOKEN_CACHE = os.path.expanduser(os.path.join(\"~\", \".cedatoken\"))\n",
    "\n",
    "\n",
    "def load_cached_token():\n",
    "    \"\"\"\n",
    "    Read the token back out from its cache file.\n",
    "\n",
    "    Returns a tuple containing the token and its expiry timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the token back out from its cache file\n",
    "    try:\n",
    "        with open(TOKEN_CACHE, \"r\") as cache_file:\n",
    "            data = json.loads(cache_file.read())\n",
    "\n",
    "            token = data.get(\"access_token\")\n",
    "            expires = datetime.strptime(data.get(\"expires\"), \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "            return token, expires\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_token():\n",
    "    \"\"\"\n",
    "    Fetches a download token, either from a cache file or\n",
    "    from the token API using CEDA login credentials.\n",
    "\n",
    "    Returns an active download token\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the cache file to see if we already have an active token\n",
    "    token, expires = load_cached_token()\n",
    "\n",
    "    # If no token has been cached or the token has expired, we get a new one\n",
    "    now = datetime.now(timezone.utc)\n",
    "    if not token or expires < now:\n",
    "\n",
    "        if not token:\n",
    "            print(f\"No previous token found at {TOKEN_CACHE}. \", end=\"\")\n",
    "        else:\n",
    "            print(f\"Token at {TOKEN_CACHE} has expired. \", end=\"\")\n",
    "        print(\"Generating a fresh token...\")\n",
    "\n",
    "        print(\"Please provide your CEDA username: \", end=\"\")\n",
    "        username = input()\n",
    "        password = getpass(prompt=\"CEDA user password: \")\n",
    "\n",
    "        credentials = b64encode(f\"{username}:{password}\".encode(\"utf-8\")).decode(\n",
    "            \"ascii\"\n",
    "        )\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Basic {credentials}\",\n",
    "        }\n",
    "        response = requests.request(\"POST\", TOKEN_URL, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # The token endpoint returns JSON\n",
    "            response_data = json.loads(response.text)\n",
    "            token = response_data[\"access_token\"]\n",
    "\n",
    "            # Store the JSON data in the cache file for future use\n",
    "            with open(TOKEN_CACHE, \"w\") as cache_file:\n",
    "                cache_file.write(response.text)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to generate token, check your username and password.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Found existing token at {TOKEN_CACHE}, skipping authentication.\")\n",
    "\n",
    "    return token, expires\n",
    "\n",
    "\n",
    "def open_datasets(urls: list[str],\n",
    "                  download_token=None\n",
    "                  ):\n",
    "    \"\"\" \n",
    "    Open a list of NetCDF datasets from specified URLs. \n",
    "    \"\"\"\n",
    "\n",
    "    datasets = []\n",
    "    headers = None\n",
    "\n",
    "    if download_token:\n",
    "        headers = {\"Authorization\": f\"Bearer {download_token}\"}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.request(\"GET\", url, headers=headers, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            print(\n",
    "                f\"Failed to fetch data. The response from the server was {response.status_code}\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        print(f\"Opening Dataset from file {filename} ...\")\n",
    "        datasets.append(Dataset(filename, memory=response.content))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def initiate_opendap_multiple_files(urls: list[str], \n",
    "                                    var_id: str\n",
    "                                    ) -> xr.Dataset:\n",
    "    \"\"\" \n",
    "    Initiate an API call to download UKCP18 data for multiple year-month selections \n",
    "\n",
    "    Returns an xarray.dataset, concatenated if necessary and chunked using dask to reduce memory usage\n",
    "    \"\"\"\n",
    "    token, expires = get_token()\n",
    "    if token:\n",
    "        # Now that we have a valid token, we can attempt to open the Dataset from a URL.\n",
    "        # This will only work if the token is associated with a CEDA user that has been granted\n",
    "        # access to the data (i.e. if they can already download the file in a browser).\n",
    "        # \n",
    "        print(f\"Fetching information about variable '{var_id}':\")\n",
    "        if token:\n",
    "            print((\n",
    "                f\"Using download token '{token[:5]}...{token[-5:]}' for authentication.\"\n",
    "                f\" Token expires at: {expires}.\"\n",
    "            ))\n",
    "        else:\n",
    "            print(\"No DOWNLOAD_TOKEN found in environment.\")\n",
    "\n",
    "        nc_datasets = open_datasets(urls, download_token=token)\n",
    "\n",
    "        xarray_datasets = []\n",
    "        for nc_data in nc_datasets:\n",
    "            if nc_data is None:\n",
    "                continue\n",
    "            ds = xr.open_dataset(xr.backends.NetCDF4DataStore(nc_data), chunks={\"time\": 30})\n",
    "            xarray_datasets.append(ds)\n",
    "\n",
    "        # combine datasets using xarray if required, and re-chunk\n",
    "        if len(xarray_datasets) > 1:\n",
    "            combined_ds = xr.concat(xarray_datasets, dim=\"time\").chunk({\"time\": 60})\n",
    "            return combined_ds\n",
    "        elif xarray_datasets:\n",
    "            return xarray_datasets[0]\n",
    "        else:\n",
    "            print(\"No datasets were opened!\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ukcp_data(input_file_path: str,\n",
    "                      output_file_path: str, \n",
    "                      year: int, \n",
    "                      month: int,\n",
    "                      member_id: int,\n",
    "                      var_id: str,\n",
    "                      mask_1D: xr.Dataset):\n",
    "    \"\"\"\n",
    "    Read UKCP18 (climate model) daily precipitation data \n",
    "\n",
    "    input_file_path: path to input file url (API)\n",
    "    output_file_path: path to output files\n",
    "    year: year to analyse (e.g. 2035)\n",
    "    month: month to analyse (e.g. 8 for August)\n",
    "    var_id: cf-compliant variable ID (e.g. pr, tas)\n",
    "    member_id: ensemble member ID (01,04,05,...,15)\n",
    "    mask_1D: xarray Dataset containing 1D land-ocean mask \n",
    "    \"\"\"\n",
    "\n",
    "    with initiate_opendap_multiple_files(input_file_path, var_id) as ds:\n",
    "        print(\"Finished reading in data from Opendap!\")\n",
    "\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "            ds = ds.stack(location=(\"grid_latitude\", \"grid_longitude\"))\n",
    "            ids = c2str.get_cell_ids(ds.location.values)\n",
    "            ds.coords['location_id'] = ('location', ids)\n",
    "            ds = ds.where(ds.bnds == 0, drop=True)\n",
    "            for item in remove_items:\n",
    "                del ds[item]\n",
    "            for item in squeeze_coords:\n",
    "                ds = ds.squeeze(item)\n",
    "\n",
    "            ds = ds.where(mask_1D[\"WCID\"] >= 0, drop=True)\n",
    "\n",
    "            starttime = timeit.default_timer()\n",
    "\n",
    "            for wcid in range(NUMBER_OF_WC):\n",
    "                print(f\"Working on water company {str(wcid)}\")\n",
    "                ds_mask = ds.where(mask_1D.WCID == wcid, drop=True)\n",
    "                for duration, start_hour in accum_duration_start.items():\n",
    "                    start = get_start_year(year, month, start_hour)\n",
    "                    precip = ds_mask.where(ds['time'] >= start, drop=True)\n",
    "                    \n",
    "                    print(\"Starting dry days calculation!\")\n",
    "                    get_dry_days(precip, year, month, member_id, wcid, output_file_path)\n",
    "                    print(\"Starting total rainfall calculation!\")\n",
    "                    get_month_total(precip, year, month, member_id, wcid, output_file_path)\n",
    "\n",
    "                    print(f\"Calculating {str(duration)}-h accumulated precip, starting at {str(start_hour)}Z\")\n",
    "                    if duration > 1:\n",
    "                        ds_window = rolling_window_sum(precip, duration)\n",
    "                        ds_window = ds_window.rename({\"pr\": \"pr_sum\"})\n",
    "                        ds_window = ds_window.assign(pr=precip.pr)\n",
    "                        ds_window['time'] = ds_window[\"time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "                        df_window = ds_window.to_dataframe()\n",
    "                        df_window.index = df_window.index.droplevel(['grid_latitude', 'grid_longitude'])\n",
    "\n",
    "                        # UNCOMMENT TO CALCULATE PRECIPITATION PROFILES\n",
    "                        # get_pr_profile(df_window, member_id, month, duration, wcid, output_file_path)\n",
    "\n",
    "                        if duration == 3 or duration == 6:\n",
    "                            get_bin_counts(df_window, year, month, member_id, duration, wcid, output_file_path)\n",
    "\n",
    "                    else:\n",
    "                        df_window = precip.to_dataframe()\n",
    "                        df_window = df_window[df_window['month_number'] == month]\n",
    "                        df_window.index = df_window.index.droplevel(['grid_latitude', 'grid_longitude'])\n",
    "                        get_pr_profile(df_window, member_id, month, duration, wcid, output_file_path)\n",
    "\n",
    "                    precip = None\n",
    "\n",
    "            print(\"This code took :\", timeit.default_timer() - starttime,\" (s) to run...\")\n",
    "\n",
    "            ds.close()\n",
    "\n",
    "\n",
    "def get_pr_profile(df_prcp_water_company: pd.DataFrame, \n",
    "                   member_id: int, \n",
    "                   month: int, \n",
    "                   duration: int, \n",
    "                   wcid: int,\n",
    "                   output_file_path: str):\n",
    "    \"\"\" \n",
    "    Identify the grid points within specified rainfall bounds (RP5, RP10, RP30, RP50, RP100) for the rolling window \n",
    "    Code is run for the grid points belonging to a single water company ('WC')\n",
    "    For each of these cases, retrieve all the data leading up to the validity time (e.g. 6-h before T+0 for a 6-h window)\n",
    "    Save this information to a dataframe and write out to a csv; repeat for water company, rolling window and RP \n",
    "    \"\"\"\n",
    "\n",
    "    # rainfall thresholds (upper,lower) for each RP within the rolling window (1-h, 3-h, 6-h)\n",
    "    PR = list(RPS[duration])\n",
    "\n",
    "    if duration > 1:\n",
    "\n",
    "        select_list = ['Time', 'location_id', 'longitude', 'latitude', 'pr', 'pr_sum']\n",
    "        final_list = ['Time', 'longitude', 'latitude', 'pr_sum', 'Hyet']\n",
    "\n",
    "        # include 'Time' as a df column rather than only the index\n",
    "        df_prcp_water_company.insert(0, 'Time', df_prcp_water_company.index)\n",
    "        # sort by 'location_id', then 'Time' and then 'pr_sum' (modifying the existing df)\n",
    "        df_prcp_water_company.sort_values(by=['location_id', 'Time', 'pr_sum'], inplace=True)\n",
    "\n",
    "        # loop over RP thresholds as defined in 'rp_years'\n",
    "        for i in range(1, len(PR) + 1):\n",
    "            filename = os.path.join(output_file_path,\n",
    "                                    f\"Profile_{rp_years[i - 1]}y_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "\n",
    "            # filter the df based on 2 conditions, and return a df containing only the filtered rows\n",
    "            # 'pr_sum' > PR[i-1] but <= PR[i] + 'month_number' == specified month  \n",
    "            if i < len(PR):\n",
    "                df_prcp_threshold = df_prcp_water_company.loc[(df_prcp_water_company['pr_sum'] > PR[i - 1]) \n",
    "                                                              & (df_prcp_water_company['pr_sum'] <= PR[i])\n",
    "                                                              & (df_prcp_water_company['month_number'] == month) ]\n",
    "            # for the highest RP there is only a lower limit (i.e. >= precip_threshold)\n",
    "            else:\n",
    "                df_prcp_threshold = df_prcp_water_company.loc[ (df_prcp_water_company['pr_sum'] > PR[i - 1]) \n",
    "                                                              & (df_prcp_water_company['month_number'] == month)]\n",
    "\n",
    "            if len(df_prcp_threshold) > 0:\n",
    "                # filter the df to only include the selected columns, for all rows [:] (see 'select_list')\n",
    "                df_prcp_threshold = df_prcp_threshold.loc[:, select_list]\n",
    "                # create a list of all the location IDs \n",
    "                location = df_prcp_threshold['location_id'].tolist()\n",
    "\n",
    "                # create a temporary df containing the columns below, from the original, unfiltered df (~5 million)\n",
    "                temp_df = df_prcp_water_company[['Time', 'location_id', 'pr', 'pr_sum']]\n",
    "                # subset by the locations that are in the list we created above (~ 20,000)\n",
    "                temp_df = temp_df[temp_df['location_id'].isin(location)]\n",
    "                # extract the values of each column individually and assign to new (temporary) variables \n",
    "                # each of these contains ~20,000 elements (for this example, 6-h rolling window)\n",
    "                temp_time = np.array(temp_df['Time'].values)\n",
    "                temp_df1 = np.array(temp_df['pr'].values)\n",
    "                temp_sum = np.array(temp_df['pr_sum'].values)\n",
    "                temp_location = np.array(temp_df['location_id'].values)\n",
    "                index_num = 0\n",
    "\n",
    "                profile_list = []\n",
    "                # loop through all the rows of the processed df ('df_pr_threshold')\n",
    "                # identify all rows where the 'pr_sum' variable matches one of the values in 'temp_sum'\n",
    "                # 'sum_index1' contains the index of each row (from the 'temp_df' dataframe)\n",
    "                for index, row in df_prcp_threshold.iterrows():\n",
    "                    this_time = row['Time']\n",
    "                    this_location = row['location_id']\n",
    "                    this_sum = row['pr_sum']\n",
    "                    sum_index = np.where(temp_sum == this_sum)\n",
    "                    sum_index1 = sum_index[0][:]\n",
    "\n",
    "                    for sum_ind in sum_index1:\n",
    "                        # 'data_profile' represents a profile of the rainfall data for the duration (6-h)\n",
    "                        # leading up to and including the index 'sum_ind'\n",
    "                        # This code grabs the rainfall values leading up to the validity time \n",
    "                        data_profile = temp_df1[sum_ind - duration + 1: sum_ind + 1].tolist()\n",
    "                        # check the location + time corresponding to the current index ('sum_ind')\n",
    "                        # if both location + time from the current df ('df_prcp_threshold') match the original df ('temp_df')\n",
    "                        # the loop breaks (condition satisfied): this code ensures that only the first match is processed \n",
    "                        if temp_location[sum_ind] == this_location and temp_time[sum_ind] == this_time:\n",
    "                            #print(f\"Match found at index {sum_ind}\")\n",
    "                            break\n",
    "\n",
    "                    profile_list.append(data_profile)\n",
    "                    index_num += 1\n",
    "\n",
    "                # add the previous 6-h of rainfall data (hyetograph) to the dataframe in the 'Hyet' column\n",
    "                df_prcp_threshold['Hyet'] = profile_list\n",
    "\n",
    "                # tidy the dataframe by keeping only the columns we need for future analysis \n",
    "                df_prcp_threshold = df_prcp_threshold.loc[:, final_list]\n",
    "                df_prcp_threshold.columns = ['end date', 'lon', 'lat', 'Total accum', 'Hyet']\n",
    "\n",
    "                # insert additional columns with WCID, ensemble member and projection slice information \n",
    "                df_prcp_threshold.insert(0, 'WCID', wcid)\n",
    "                df_prcp_threshold.insert(0, 'Member', member_id)\n",
    "                df_prcp_threshold.insert(0, 'Projection_slice_ID', PROJECTION_ID)\n",
    "                save(filename, df_prcp_threshold)\n",
    "\n",
    "                df_prcp_threshold = None\n",
    "\n",
    "    else:\n",
    "        for i in range(1, len(PR) + 1):\n",
    "            filename = os.path.join(output_file_path,\n",
    "                                    f\"Profile_{rp_years[i - 1]}y_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "            if i < len(PR):\n",
    "                df_prcp_threshold = df_prcp_water_company[(df_prcp_water_company['pr'] > PR[i - 1]) & (df_prcp_water_company['pr'] <= PR[i])]\n",
    "            else:\n",
    "                df_prcp_threshold = df_prcp_water_company[df_prcp_water_company['pr'] > PR[i - 1]]\n",
    "\n",
    "            df_prcp_threshold = df_prcp_threshold.loc[:, ['latitude', 'longitude', 'pr']]\n",
    "\n",
    "            df_prcp_threshold.insert(0, 'Time', df_prcp_threshold.index)\n",
    "            df_prcp_threshold.dropna(subset=[\"pr\"], inplace=True)\n",
    "            df_prcp_threshold.columns = ['end date', 'lon', 'lat', 'pr_sum']\n",
    "\n",
    "            df_prcp_threshold.insert(0, 'WCID', wcid)\n",
    "            df_prcp_threshold.insert(0, 'Member', member_id)\n",
    "            df_prcp_threshold.insert(0, 'Projection_slice_ID', PROJECTION_ID)\n",
    "\n",
    "            save(filename, df_prcp_threshold)\n",
    "            df_prcp_threshold = None\n",
    "\n",
    "\n",
    "def str_to_cftime360(time_str: str) -> cftime:\n",
    "    \"\"\" \n",
    "    Apply string to cftime360 conversion to each item in an iterable \n",
    "    Turn '2024-09-15 00:30' into '2024-09-15-00-30' and then split by '-'\n",
    "    Final result: ['2024', '09', '15', '00', '30']\n",
    "    \"\"\"\n",
    "    year, month, day, hour, minute = map(int, time_str.replace(\":\", \"-\").replace(\" \", \"-\").split(\"-\"))\n",
    "    return cftime.Datetime360Day(year, month, day, hour, minute)\n",
    "\n",
    "\n",
    "def get_bin_counts(df_prcp: pd.DataFrame, \n",
    "                   year: int, \n",
    "                   month: int, \n",
    "                   member_id: int, \n",
    "                   duration: int, \n",
    "                   wcid: int,\n",
    "                   output_file_path: str):\n",
    "    \"\"\"\n",
    "    Calculate rainfall counts for specified bins relevant to the chosen event duration (e.g. 1-h, 3-h, 6-h)\n",
    "    For September, the first and second halves are counted separately for a reason that Kay explained to be (but I've forgotten)\n",
    "    \"\"\"\n",
    "    bins = BINS[duration]\n",
    "    filename = os.path.join(output_file_path, f\"Rainfall_bin_counts_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Bin counts']\n",
    "\n",
    "    total_count = []\n",
    "\n",
    "    if month == 9:\n",
    "        df_prcp_copy = df_prcp\n",
    "        df_prcp_copy.insert(0, 'Time', df_prcp_copy.index)\n",
    "        df_prcp_copy['Time'] = df_prcp_copy['Time'].apply(str_to_cftime360)\n",
    "        mid_sept_date = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "        start_sept_date = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "        df_prcp_copy = df_prcp_copy[(df_prcp_copy['Time'] >= start_sept_date) & (df_prcp_copy['Time'] <= mid_sept_date)]\n",
    "        for i in range(1, len(bins)):\n",
    "            df_count = df_prcp_copy[(df_prcp_copy['pr_sum'] > bins[i - 1]) & (df_prcp_copy['pr_sum'] < bins[i])]\n",
    "            total_count.append(df_count.shape[0])\n",
    "        data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "        total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "        save(filename, total_count_df)\n",
    "\n",
    "        data_list = None\n",
    "        total_count = None\n",
    "        total_count_df = None\n",
    "    \n",
    "    else:\n",
    "        for i in range(1, len(bins)):\n",
    "            df_count = df_prcp[(df_prcp['pr_sum'] > bins[i - 1]) & (df_prcp['pr_sum'] < bins[i])]\n",
    "            total_count.append(df_count.shape[0])\n",
    "        data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "        total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "        save(filename, total_count_df)\n",
    "\n",
    "        data_list = None\n",
    "        total_count = None\n",
    "        total_count_df = None\n",
    "\n",
    "def get_dry_days(ds_precip: xr.Dataset,\n",
    "                 year: int, \n",
    "                 month: int, \n",
    "                 member_id: int, \n",
    "                 wcid: int,\n",
    "                 output_file_path: str):\n",
    "    \"\"\"\n",
    "    Dry day counts from UKCP18 daily precipitation data (xr.ds)\n",
    "    Calculate the number of dry days per month for each grid point, and then calculate the mean\n",
    "    Calls `save_dry_counts` to write data out to csv file \n",
    "    \"\"\"\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        if month == 9:\n",
    "            start_sept = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "            ds_sept = ds_precip.where((ds_precip['time'] <= start_sept) & (ds_precip['time'].dt.month == month), drop=True)\n",
    "            daily_precip = ds_sept['pr'].resample(time=\"D\").sum()\n",
    "            da_dry_days_sept = daily_precip.where(daily_precip < 0.1)\n",
    "            da_dry_day_count_sept = da_dry_days_sept.count(dim=\"time\")\n",
    "            save_dry_counts(da_dry_day_count_sept, year, 13, member_id, wcid, output_file_path)\n",
    "        else:\n",
    "            ds_month = ds_precip.where(ds_precip['time'].dt.month == month, drop=True)\n",
    "            daily_precip = ds_month['pr'].resample(time=\"D\").sum()\n",
    "            da_dry_days = daily_precip.where(daily_precip < 0.1)\n",
    "            da_dry_day_count = da_dry_days.count(dim=\"time\")\n",
    "            save_dry_counts(da_dry_day_count, year, month, member_id, wcid, output_file_path)\n",
    "\n",
    "\n",
    "def get_month_total(ds_precip: xr.Dataset, \n",
    "                    year: int, \n",
    "                    month: int, \n",
    "                    member_id: int, \n",
    "                    wcid: int,\n",
    "                    output_file_path: str):\n",
    "    \"\"\"\n",
    "    This function calculates total monthly precip\n",
    "    Calls `save_month_total' to write data out to csv file \n",
    "    \"\"\"\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        if month == 9:\n",
    "            start_sept = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "            ds_sept = ds_precip.where((ds_precip['time'] <= start_sept) & (ds_precip['time'].dt.month == month), drop=True)\n",
    "            ds_sept_total = ds_sept.sum(dim='time')\n",
    "            save_month_total(ds_sept_total, year, 13, member_id, wcid, output_file_path)\n",
    "        else:\n",
    "            start = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "            ds_month = ds_precip.where(ds_precip['time'] >= start, drop=True)\n",
    "            ds_month_total = ds_month.sum(dim='time')\n",
    "            save_month_total(ds_month_total, year, month, member_id, wcid, output_file_path)\n",
    "\n",
    "\n",
    "def save_dry_counts(ds_dry_days: xr.Dataset, \n",
    "                    year: int, \n",
    "                    month: int, \n",
    "                    member_id: int, \n",
    "                    wcid: int,\n",
    "                    output_file_path: str):\n",
    "    \"\"\" \n",
    "    Calculate the mean number of dry days in a given month over all grid points \n",
    "    Save this dry day count data to a csv file \n",
    "    Called by `get_dry_days`\n",
    "    \"\"\"\n",
    "    filename = os.path.join(output_file_path, f\"Dry_days_counts_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "    ds_mask = ds_dry_days.mean()\n",
    "    dry_list = [PROJECTION_ID, member_id, year, month, wcid, ds_mask.values.tolist()]\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Mean dry day counts']\n",
    "    dry_days_df = pd.DataFrame([dry_list], columns=cols)\n",
    "    save(filename, dry_days_df)\n",
    "\n",
    "\n",
    "def save_month_total(ds_month_total: xr.Dataset, \n",
    "                     year: int, \n",
    "                     month: int, \n",
    "                     member_id: int, \n",
    "                     wcid: int,\n",
    "                     output_file_path: str):\n",
    "    \"\"\" \n",
    "    Save total monthly precip to a csv file \n",
    "    Called by `get_month_total` \n",
    "    \"\"\"\n",
    "    filename = os.path.join(output_file_path, f\"Total_rainfall_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "    ds_mask = ds_month_total.mean()\n",
    "    month_total_list = [PROJECTION_ID, member_id, year, month, wcid, ds_mask.pr.values.tolist()]\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Mean total rainfall']\n",
    "    month_total_df = pd.DataFrame([month_total_list], columns=cols)\n",
    "    save(filename, month_total_df)\n",
    "\n",
    "\n",
    "def get_previous_month_url(url: str) -> Tuple[int, int, str]: \n",
    "    \"\"\" \n",
    "    Extract month and year from a UKCP url, and output another url for the previous month \n",
    "    \"\"\"\n",
    "    match = re.search(r\"(\\d{4})(\\d{2})\\d{2}-\\d{8}\", url)\n",
    "    if match:\n",
    "        year = int(match.group(1))  # Extracted year\n",
    "        month = int(match.group(2))  # Extracted month\n",
    "\n",
    "        # Create a datetime object for the extracted year and month\n",
    "        current_date = datetime(year, month, 1)\n",
    "\n",
    "        # Subtract one month\n",
    "        previous_month_date = current_date - relativedelta(months=1)\n",
    "\n",
    "        # Get year and month of the previous month\n",
    "        previous_year = previous_month_date.year\n",
    "        previous_month = previous_month_date.month\n",
    "\n",
    "        # Format the result as \"YYYY-MM\"\n",
    "        previous_month_str = previous_month_date.strftime(\"%Y%m\")\n",
    "        updated_url = re.sub(r\"(\\d{4}\\d{2})\\d{2}-\\d{8}\", f\"{previous_month_str}01-{previous_month_str}30\", url)\n",
    "        return previous_year, previous_month, updated_url\n",
    "    else:\n",
    "        raise ValueError(\"Date not found in URL!\")\n",
    "\n",
    "\n",
    "def get_start_year(year: int, \n",
    "                   month: int, \n",
    "                   hour: int):\n",
    "    \"\"\" \n",
    "    This function provides a buffer around the selected date\n",
    "    Starts the analysis on the 30th of the previous month\n",
    "    (i.e. 30th June 1981 if the user chose July 1981)\n",
    "    \"\"\"\n",
    "    if year != 1980:\n",
    "        year1 = year #1981\n",
    "        month1 = month - 1 #6\n",
    "        if month == 1:\n",
    "            year1 = year - 1\n",
    "            month1 = 12\n",
    "        start = cftime.Datetime360Day(year1, month1, 30, hour, 0, 0)\n",
    "    else:\n",
    "        month = 12\n",
    "        start = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "\n",
    "    return start\n",
    "\n",
    "\n",
    "def call_main(proj_df: pd.DataFrame, \n",
    "              ukcp_url: str,\n",
    "              output_file_path: str,\n",
    "              member_id: int,\n",
    "              var_id: str,\n",
    "              mask_1D: xr.Dataset\n",
    "              ):\n",
    "    \"\"\" \n",
    "    Call the main function to read in UKCP data for the chosen year and month \n",
    "    \"\"\"\n",
    "    global PROJECTION_ID\n",
    "\n",
    "    year, month, previous_month_url = get_previous_month_url(ukcp_url)\n",
    "\n",
    "    df_row = proj_df[(proj_df['Month'] == month) & (proj_df['Year'] == year)]\n",
    "    if df_row.empty:\n",
    "        print(f\"No data found for Month: {month}, Year: {year}\")\n",
    "\n",
    "    if (year==1980 and month==12) or (year==2020 and month==12) or (year==2060 and month==12):\n",
    "        input_file_path = [ukcp_url]\n",
    "    else:\n",
    "        input_file_path = [previous_month_url, ukcp_url]\n",
    "\n",
    "    process_ukcp_data(input_file_path, \n",
    "                      output_file_path,\n",
    "                      year, \n",
    "                      month, \n",
    "                      member_id, \n",
    "                      var_id, \n",
    "                      mask_1D)\n",
    "\n",
    "\n",
    "def check_dir(file_name: str):\n",
    "    \"\"\" \n",
    "    check if a directory exists, and create one if not \n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(file_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save(file_name: str, \n",
    "         df: pd.DataFrame\n",
    "         ):\n",
    "    \"\"\" \n",
    "    save pandas dataframe as a csv \n",
    "    \"\"\"\n",
    "    check_dir(file_name)\n",
    "    if os.path.isfile(file_name):\n",
    "        df.to_csv(file_name, mode='a', header=False, index=False, float_format=\"%.2f\")\n",
    "    else:\n",
    "        df.to_csv(file_name, mode='a', index=False, float_format=\"%.2f\")\n",
    "\n",
    "\n",
    "def rolling_window_sum(ds: xr.Dataset, \n",
    "                       window_size: int\n",
    "                       ) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    rolling window calculation for an xr.ds by defined window size (1-h, 3-h, 6-h, etc)\n",
    "    \"\"\"\n",
    "    print(f\"Starting calculation of rolling {str(window_size)}-h accumulated precip!\")\n",
    "    ds_window = ds.rolling(time=window_size, min_periods=window_size).construct(\"new\").sum(\"new\", skipna=True)\n",
    "    print(f\"Finished calculating rolling {str(window_size)}-h accumulated precip!\")\n",
    "\n",
    "    return ds_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --ukcp_url path --out_file_path path\n",
      "                             --projection_id value (int) --ensemble_member_id\n",
      "                             value (int) --variable_id value (str)\n",
      "ipykernel_launcher.py: error: the following arguments are required: --ukcp_url, --out_file_path, --projection_id, --ensemble_member_id, --variable_id\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbanorthwest.co.uk/samhardy/miniforge3/envs/2024s1475-env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def call_api_and_process_ukcp_data(ukcp_url: str, \n",
    "                                   out_file_path: str,\n",
    "                                   projection_id: int,\n",
    "                                   ensemble_member_id: int,\n",
    "                                   variable_id: str): \n",
    "    \"\"\" \n",
    "    Process UKCP18 daily precipitation data for a given date (month + year), ensemble member and time slice \n",
    "    \"\"\"\n",
    "    global PROJECTION_ID\n",
    "    PROJECTION_ID = projection_id\n",
    "\n",
    "    mask_nc_filename = \"UKWC_Cleaned_land-cpm_uk_2.2km.nc\"\n",
    "    csv_filename = \"YearsMonths_byBinCounts_Rand_OtherYears.csv\"\n",
    "\n",
    "    profile_selected_month = pd.read_csv(os.path.join(\"./\", csv_filename))\n",
    "    mask_nc = os.path.join(\"./\", mask_nc_filename)\n",
    "    mask_orig = xr.open_dataset(mask_nc)\n",
    "    mask_1D = mask_orig.stack(location=(\"grid_latitude\", \"grid_longitude\"))\n",
    "\n",
    "    projection_profile = profile_selected_month[profile_selected_month['Projection_slice_ID']\n",
    "                                                == projection_id]\n",
    "    \n",
    "    call_main(projection_profile, \n",
    "              ukcp_url, \n",
    "              out_file_path, \n",
    "              ensemble_member_id, \n",
    "              variable_id, \n",
    "              mask_1D)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Download and process UKCP18 daily precipitation data using the CEDA API')\n",
    "    parser.add_argument('--ukcp_url', metavar='path', required=True, \n",
    "                        help='URL to the UKCP18 data')\n",
    "    parser.add_argument('--out_file_path', metavar='path', required=True,\n",
    "                        help='output file path')\n",
    "    parser.add_argument('--projection_id', metavar='value (int)', required=True,\n",
    "                        help='climate simulation projection slice ID')\n",
    "    parser.add_argument('--ensemble_member_id', metavar='value (int)', required=True,\n",
    "                        help='ensemble member ID (1,4,5,...,12,13,15)')\n",
    "    parser.add_argument('--variable_id', metavar='value (str)', required=True,\n",
    "                        help='cf-compliant UKCP variable ID (e.g. pr,tas)')\n",
    "    args = parser.parse_args()\n",
    "    call_api_and_process_ukcp_data(ukcp_url=args.ukcp_url, \n",
    "                                   out_file_path=args.out_file_path,\n",
    "                                   projection_id=args.projection_id, \n",
    "                                   ensemble_member_id=args.ensemble_member_id,\n",
    "                                   variable_id=args.variable_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024s1475-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
