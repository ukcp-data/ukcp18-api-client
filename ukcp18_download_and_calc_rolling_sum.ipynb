{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UKCP 18 rainfall processing\n",
    "\n",
    "- Author: Sam Hardy (modified from Changgui Wang's original Python code)\n",
    "\n",
    "- required input from user:\n",
    "    - `csv_filename`: `\"YearsMonths_byBinCounts_Rand_OtherYears.csv\"`\n",
    "    - `mask_nc_filename`: `\"UKWC_Cleaned_land-cpm_uk_2.2km.nc\"`\n",
    "    - `year`: e.g. 2024\n",
    "    - `month`: e.g. 12 \n",
    "    - `ensemble_member_id`: e.g. 4 \n",
    "\n",
    "- both files are read from the `INPUT` directory set at the begining of the script: \n",
    "    - INPUT_PATH = home directory (previously OneDrive folder on local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and local routines \n",
    "\n",
    "- import local routine: `convert_ll2str.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.config\n",
    "import cftime\n",
    "import timeit\n",
    "import convert_ll2str as c2str\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from base64 import b64encode\n",
    "from datetime import datetime, timezone\n",
    "from getpass import getpass\n",
    "from netCDF4 import Dataset\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "\n",
    "- Calculating 1-h, 3-h and 6-h accumulated precipitation \n",
    "- Calculating data for 13 water companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2035\n",
    "month = 9\n",
    "ensemble_member_id = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = str(Path.home())\n",
    "\n",
    "mask_nc_filename = \"UKWC_Cleaned_land-cpm_uk_2.2km.nc\"\n",
    "csv_filename = \"YearsMonths_byBinCounts_Rand_OtherYears.csv\"\n",
    "projection_id = 2 # 2021-2040 time slice\n",
    "var_id = 'pr'\n",
    "\n",
    "BINS = {1: [2, 4, 7, 10, 14, 18, 24, 30, 40, 55, 70, 90, 110, 135],\n",
    "        3: [2, 6, 10, 15, 20, 30, 40, 60, 80, 110, 140, 175, 215, 265],\n",
    "        6: [2, 7, 13, 19, 28, 40, 55, 80, 115, 160, 210, 260, 320, 390]}\n",
    "\n",
    "# these values represent the rainfall thresholds for each interval (1-h, 3-h, 6-h) corresponding to different RPs \n",
    "RPS = {1: [19, 24, 32, 36, 42], \n",
    "       3: [29, 35, 44, 49, 57],\n",
    "       6: [35, 42, 53, 59, 67]}\n",
    "\n",
    "rp_years = [5, 10, 30, 50, 100]\n",
    "\n",
    "global OUTPUT_PATH\n",
    "global PROJECTION_ID\n",
    "\n",
    "INPUT_PATH = './'\n",
    "SAVE_PATH = '/mnt/metdata/2024s1475/Dry_Days_Total_Rainfall_Dec2024'\n",
    "NUMBER_OF_WC = 13\n",
    "\n",
    "accum_duration_start = {1: 23, 3: 22, 6: 19}\n",
    "MASK = None\n",
    "\n",
    "remove_items = ['ensemble_member_id', 'grid_latitude_bnds', 'grid_longitude_bnds',\n",
    "                'time_bnds','rotated_latitude_longitude', 'year', 'yyyymmddhh', 'ensemble_member']\n",
    "squeeze_coords = [\"bnds\", \"ensemble_member\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for retrieving UKCP data using an API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "remote_nc_with_token.py\n",
    "===================\n",
    "\n",
    "Python script for reading a NetCDF file remotely from the CEDA archive. It demonstrates fetching\n",
    "and using a download token to authenticate access to CEDA Archive data, as well as how to load\n",
    "and subset the Dataset from a stream of data (diskless), without having to download the whole file.\n",
    "\n",
    "Pre-requisites:\n",
    "\n",
    " - Python3.x\n",
    " - Python libraries (installed by Pip):\n",
    "\n",
    "```\n",
    "netCDF4\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```\n",
    "$ python remote_nc_with_token.py <url> <var_id>\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "$ URL=https://dap.ceda.ac.uk/badc/ukcp18/data/marine-sim/skew-trend/rcp85/skewSurgeTrend/latest/skewSurgeTrend_marine-sim_rcp85_trend_2007-2099.nc\n",
    "$ VAR_ID=skewSurgeTrend\n",
    "\n",
    "$ python remote_nc_with_token.py $URL $VAR_ID\n",
    "```\n",
    "\n",
    "You will be prompted to provide your CEDA username and password the first time the script is run and\n",
    "again if the token cached from a previous attempt has expired.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# URL for the CEDA Token API service\n",
    "TOKEN_URL = \"https://services-beta.ceda.ac.uk/api/token/create/\"\n",
    "# Location on the filesystem to store a cached download token\n",
    "TOKEN_CACHE = os.path.expanduser(os.path.join(\"~\", \".cedatoken\"))\n",
    "\n",
    "\n",
    "def load_cached_token():\n",
    "    \"\"\"\n",
    "    Read the token back out from its cache file.\n",
    "\n",
    "    Returns a tuple containing the token and its expiry timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the token back out from its cache file\n",
    "    try:\n",
    "        with open(TOKEN_CACHE, \"r\") as cache_file:\n",
    "            data = json.loads(cache_file.read())\n",
    "\n",
    "            token = data.get(\"access_token\")\n",
    "            expires = datetime.strptime(data.get(\"expires\"), \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "            return token, expires\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_token():\n",
    "    \"\"\"\n",
    "    Fetches a download token, either from a cache file or\n",
    "    from the token API using CEDA login credentials.\n",
    "\n",
    "    Returns an active download token\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the cache file to see if we already have an active token\n",
    "    token, expires = load_cached_token()\n",
    "\n",
    "    # If no token has been cached or the token has expired, we get a new one\n",
    "    now = datetime.now(timezone.utc)\n",
    "    if not token or expires < now:\n",
    "\n",
    "        if not token:\n",
    "            print(f\"No previous token found at {TOKEN_CACHE}. \", end=\"\")\n",
    "        else:\n",
    "            print(f\"Token at {TOKEN_CACHE} has expired. \", end=\"\")\n",
    "        print(\"Generating a fresh token...\")\n",
    "\n",
    "        print(\"Please provide your CEDA username: \", end=\"\")\n",
    "        username = input()\n",
    "        password = getpass(prompt=\"CEDA user password: \")\n",
    "\n",
    "        credentials = b64encode(f\"{username}:{password}\".encode(\"utf-8\")).decode(\n",
    "            \"ascii\"\n",
    "        )\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Basic {credentials}\",\n",
    "        }\n",
    "        response = requests.request(\"POST\", TOKEN_URL, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # The token endpoint returns JSON\n",
    "            response_data = json.loads(response.text)\n",
    "            token = response_data[\"access_token\"]\n",
    "\n",
    "            # Store the JSON data in the cache file for future use\n",
    "            with open(TOKEN_CACHE, \"w\") as cache_file:\n",
    "                cache_file.write(response.text)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to generate token, check your username and password.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Found existing token at {TOKEN_CACHE}, skipping authentication.\")\n",
    "\n",
    "    return token, expires\n",
    "\n",
    "\n",
    "def open_datasets(urls: list[str],\n",
    "                  download_token=None\n",
    "                  ):\n",
    "    \"\"\" \n",
    "    Open a list of NetCDF datasets from specified URLs. \n",
    "    \"\"\"\n",
    "\n",
    "    datasets = []\n",
    "    headers = None\n",
    "\n",
    "    if download_token:\n",
    "        headers = {\"Authorization\": f\"Bearer {download_token}\"}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.request(\"GET\", url, headers=headers, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            print(\n",
    "                f\"Failed to fetch data. The response from the server was {response.status_code}\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        print(f\"Opening Dataset from file {filename} ...\")\n",
    "        datasets.append(Dataset(filename, memory=response.content))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def initiate_opendap_multiple_files(urls: list[str], \n",
    "                                    var_id: str\n",
    "                                    ) -> xr.Dataset:\n",
    "    \"\"\" \n",
    "    Initiate an API call to download UKCP18 data for multiple year-month selections \n",
    "\n",
    "    Returns an xarray.dataset, concatenated if necessary and chunked using dask to reduce memory usage\n",
    "    \"\"\"\n",
    "    token, expires = get_token()\n",
    "    if token:\n",
    "        # Now that we have a valid token, we can attempt to open the Dataset from a URL.\n",
    "        # This will only work if the token is associated with a CEDA user that has been granted\n",
    "        # access to the data (i.e. if they can already download the file in a browser).\n",
    "        # \n",
    "        print(f\"Fetching information about variable '{var_id}':\")\n",
    "        if token:\n",
    "            print((\n",
    "                f\"Using download token '{token[:5]}...{token[-5:]}' for authentication.\"\n",
    "                f\" Token expires at: {expires}.\"\n",
    "            ))\n",
    "        else:\n",
    "            print(\"No DOWNLOAD_TOKEN found in environment.\")\n",
    "\n",
    "        nc_datasets = open_datasets(urls, download_token=token)\n",
    "\n",
    "        xarray_datasets = []\n",
    "        for nc_data in nc_datasets:\n",
    "            if nc_data is None:\n",
    "                continue\n",
    "\n",
    "            # print(\"\\n[INFO] Global attributes:\")\n",
    "            # for attr in nc_data.ncattrs():\n",
    "            #     print(\"\\t{}: {}\".format(attr, nc_data.getncattr(attr)))\n",
    "\n",
    "            # print(\"\\n[INFO] Variables:\\n{}\".format(nc_data.variables))\n",
    "            # print(\"\\n[INFO] Dimensions:\\n{}\".format(nc_data.dimensions))\n",
    "\n",
    "            # print(\"\\n[INFO] Max and min variable: {}\".format(var_id))\n",
    "            # variable = nc_data.variables[var_id][:]\n",
    "            # units = nc_data.variables[var_id].units\n",
    "            # print(\n",
    "            #     \"\\tMin: {:.6f} {}; Max: {:.6f} {}\".format(\n",
    "            #         variable.min(), units, variable.max(), units\n",
    "            #     )\n",
    "            # )\n",
    "            # return nc_data\n",
    "            ds = xr.open_dataset(xr.backends.NetCDF4DataStore(nc_data), chunks={\"time\": 30})\n",
    "            xarray_datasets.append(ds)\n",
    "\n",
    "        # combine datasets using xarray if required \n",
    "        if len(xarray_datasets) > 1:\n",
    "            combined_ds = xr.concat(xarray_datasets, dim=\"time\").chunk({\"time\": 30})\n",
    "            return combined_ds\n",
    "        elif xarray_datasets:\n",
    "            return xarray_datasets[0]\n",
    "        else:\n",
    "            print(\"No datasets were opened!\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(infile: str, \n",
    "         year: int, \n",
    "         month: int,\n",
    "         member_id: int,\n",
    "         mask_1D: xr.Dataset):\n",
    "    \"\"\"\n",
    "    Read UKCP18 climate change data precipitation\n",
    "    infile: input file list\n",
    "    year: start year\n",
    "    \"\"\"\n",
    "\n",
    "    with initiate_opendap_multiple_files(infile,var_id) as ds:\n",
    "        print(\"Finished reading in data from Opendap!\")\n",
    "\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "            ds = ds.stack(location=(\"grid_latitude\", \"grid_longitude\"))\n",
    "            ids = c2str.get_cell_ids(ds.location.values)\n",
    "            ds.coords['location_id'] = ('location', ids)\n",
    "            ds = ds.where(ds.bnds == 0, drop=True)\n",
    "            for item in remove_items:\n",
    "                del ds[item]\n",
    "            for item in squeeze_coords:\n",
    "                ds = ds.squeeze(item)\n",
    "\n",
    "            ds = ds.where(mask_1D[\"WCID\"] >= 0, drop=True)\n",
    "\n",
    "            starttime = timeit.default_timer()\n",
    "\n",
    "            for wcid in range(NUMBER_OF_WC):\n",
    "                print(f\"Working on water company {str(wcid)}\")\n",
    "                ds_mask = ds.where(mask_1D.WCID == wcid, drop=True)\n",
    "                for duration, start_hour in accum_duration_start.items():\n",
    "                    start = get_start_year(year, month, start_hour)\n",
    "                    precip = ds_mask.where(ds['time'] >= start, drop=True)\n",
    "                    \n",
    "                    print(\"Starting dry days calculation!\")\n",
    "                    get_dry_days(precip, year, month, member_id, wcid)\n",
    "\n",
    "                    print(f\"Calculating {str(duration)}-h accumulated precip, starting at {str(start_hour)}Z\")\n",
    "                    if duration > 1:\n",
    "                        ds_window = rolling_window_sum(precip, duration)\n",
    "                        ds_window = ds_window.rename({\"pr\": \"pr_sum\"})\n",
    "                        ds_window = ds_window.assign(pr=precip.pr)\n",
    "                        ds_window['time'] = ds_window[\"time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "                        df_window = ds_window.to_dataframe()\n",
    "                        df_window.index = df_window.index.droplevel(['grid_latitude', 'grid_longitude'])\n",
    "\n",
    "                        # UNCOMMENT TO CALCULATE PRECIPITATION PROFILES\n",
    "                        # get_pr_profile(df_window, member_id, month, duration, wcid)\n",
    "\n",
    "                        if duration == 3 or duration == 6:\n",
    "                            get_bin_counts(df_window, year, month, member_id, duration, wcid)\n",
    "\n",
    "                    else:\n",
    "                        df_window = precip.to_dataframe()\n",
    "                        df_window = df_window[df_window['month_number'] == month]\n",
    "                        df_window.index = df_window.index.droplevel(['grid_latitude', 'grid_longitude'])\n",
    "                        get_pr_profile(df_window, member_id, month, duration, wcid)\n",
    "\n",
    "                    precip = None\n",
    "                    df_window_duration = None\n",
    "\n",
    "            print(\"This code took :\", timeit.default_timer() - starttime,\" (s) to run...\")\n",
    "\n",
    "            ds.close()\n",
    "\n",
    "\n",
    "def get_pr_profile(df_prcp_water_company: pd.DataFrame, \n",
    "                   member_id: int, \n",
    "                   month: int, \n",
    "                   duration: int, \n",
    "                   wcid: int):\n",
    "    \"\"\" \n",
    "    Identify the grid points within specified rainfall bounds (RP5, RP10, RP30, RP50, RP100) for the rolling window \n",
    "    Code is run for the grid points belonging to a single water company ('WC')\n",
    "    For each of these cases, retrieve all the data leading up to the validity time (e.g. 6-h before T+0 for a 6-h window)\n",
    "    Save this information to a dataframe and write out to a csv; repeat for water company, rolling window and RP \n",
    "    \"\"\"\n",
    "\n",
    "    # rainfall thresholds (upper,lower) for each RP within the rolling window (1-h, 3-h, 6-h)\n",
    "    PR = list(RPS[duration])\n",
    "\n",
    "    if duration > 1:\n",
    "\n",
    "        select_list = ['Time', 'location_id', 'longitude', 'latitude', 'pr', 'pr_sum']\n",
    "        final_list = ['Time', 'longitude', 'latitude', 'pr_sum', 'Hyet']\n",
    "\n",
    "        # include 'Time' as a df column rather than only the index\n",
    "        df_prcp_water_company.insert(0, 'Time', df_prcp_water_company.index)\n",
    "        # sort by 'location_id', then 'Time' and then 'pr_sum' (modifying the existing df)\n",
    "        df_prcp_water_company.sort_values(by=['location_id', 'Time', 'pr_sum'], inplace=True)\n",
    "\n",
    "        # loop over RP thresholds as defined in 'rp_years'\n",
    "        for i in range(1, len(PR) + 1):\n",
    "            filename = os.path.join(OUTPUT_PATH,\n",
    "                                    f\"Profile_{rp_years[i - 1]}y_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "\n",
    "            # filter the df based on 2 conditions, and return a df containing only the filtered rows\n",
    "            # 'pr_sum' > PR[i-1] but <= PR[i] + 'month_number' == specified month  \n",
    "            if i < len(PR):\n",
    "                df_prcp_threshold = df_prcp_water_company.loc[(df_prcp_water_company['pr_sum'] > PR[i - 1]) \n",
    "                                                              & (df_prcp_water_company['pr_sum'] <= PR[i])\n",
    "                                                              & (df_prcp_water_company['month_number'] == month) ]\n",
    "            # for the highest RP there is only a lower limit (i.e. >= precip_threshold)\n",
    "            else:\n",
    "                df_prcp_threshold = df_prcp_water_company.loc[ (df_prcp_water_company['pr_sum'] > PR[i - 1]) \n",
    "                                                              & (df_prcp_water_company['month_number'] == month)]\n",
    "\n",
    "            if len(df_prcp_threshold) > 0:\n",
    "                # filter the df to only include the selected columns, for all rows [:] (see 'select_list')\n",
    "                df_prcp_threshold = df_prcp_threshold.loc[:, select_list]\n",
    "                # create a list of all the location IDs \n",
    "                location = df_prcp_threshold['location_id'].tolist()\n",
    "\n",
    "                # create a temporary df containing the columns below, from the original, unfiltered df (~5 million)\n",
    "                temp_df = df_prcp_water_company[['Time', 'location_id', 'pr', 'pr_sum']]\n",
    "                # subset by the locations that are in the list we created above (~ 20,000)\n",
    "                temp_df = temp_df[temp_df['location_id'].isin(location)]\n",
    "                # extract the values of each column individually and assign to new (temporary) variables \n",
    "                # each of these contains ~20,000 elements (for this example, 6-h rolling window)\n",
    "                temp_time = np.array(temp_df['Time'].values)\n",
    "                temp_df1 = np.array(temp_df['pr'].values)\n",
    "                temp_sum = np.array(temp_df['pr_sum'].values)\n",
    "                temp_location = np.array(temp_df['location_id'].values)\n",
    "                index_num = 0\n",
    "\n",
    "                profile_list = []\n",
    "                # loop through all the rows of the processed df ('df_pr_threshold')\n",
    "                # identify all rows where the 'pr_sum' variable matches one of the values in 'temp_sum'\n",
    "                # 'sum_index1' contains the index of each row (from the 'temp_df' dataframe)\n",
    "                for index, row in df_prcp_threshold.iterrows():\n",
    "                    this_time = row['Time']\n",
    "                    this_location = row['location_id']\n",
    "                    this_sum = row['pr_sum']\n",
    "                    sum_index = np.where(temp_sum == this_sum)\n",
    "                    sum_index1 = sum_index[0][:]\n",
    "\n",
    "                    for sum_ind in sum_index1:\n",
    "                        # 'data_profile' represents a profile of the rainfall data for the duration (6-h)\n",
    "                        # leading up to and including the index 'sum_ind'\n",
    "                        # This code grabs the rainfall values leading up to the validity time \n",
    "                        data_profile = temp_df1[sum_ind - duration + 1: sum_ind + 1].tolist()\n",
    "                        # check the location + time corresponding to the current index ('sum_ind')\n",
    "                        # if both location + time from the current df ('df_prcp_threshold') match the original df ('temp_df')\n",
    "                        # the loop breaks (condition satisfied): this code ensures that only the first match is processed \n",
    "                        if temp_location[sum_ind] == this_location and temp_time[sum_ind] == this_time:\n",
    "                            #print(f\"Match found at index {sum_ind}\")\n",
    "                            break\n",
    "\n",
    "                    profile_list.append(data_profile)\n",
    "                    index_num += 1\n",
    "\n",
    "                # add the previous 6-h of rainfall data (hyetograph) to the dataframe in the 'Hyet' column\n",
    "                df_prcp_threshold['Hyet'] = profile_list\n",
    "\n",
    "                # tidy the dataframe by keeping only the columns we need for future analysis \n",
    "                df_prcp_threshold = df_prcp_threshold.loc[:, final_list]\n",
    "                df_prcp_threshold.columns = ['end date', 'lon', 'lat', 'Total accum', 'Hyet']\n",
    "\n",
    "                # insert additional columns with WCID, ensemble member and projection slice information \n",
    "                df_prcp_threshold.insert(0, 'WCID', wcid)\n",
    "                df_prcp_threshold.insert(0, 'Member', member_id)\n",
    "                df_prcp_threshold.insert(0, 'Projection_slice_ID', PROJECTION_ID)\n",
    "                save(filename, df_prcp_threshold)\n",
    "\n",
    "                df_prcp_threshold = None\n",
    "\n",
    "    else:\n",
    "        for i in range(1, len(PR) + 1):\n",
    "            filename = os.path.join(OUTPUT_PATH,\n",
    "                                    f\"Profile_{rp_years[i - 1]}y_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "            if i < len(PR):\n",
    "                df_prcp_threshold = df_prcp_water_company[(df_prcp_water_company['pr'] > PR[i - 1]) & (df_prcp_water_company['pr'] <= PR[i])]\n",
    "            else:\n",
    "                df_prcp_threshold = df_prcp_water_company[df_prcp_water_company['pr'] > PR[i - 1]]\n",
    "\n",
    "            df_prcp_threshold = df_prcp_threshold.loc[:, ['latitude', 'longitude', 'pr']]\n",
    "\n",
    "            df_prcp_threshold.insert(0, 'Time', df_prcp_threshold.index)\n",
    "            df_prcp_threshold.dropna(subset=[\"pr\"], inplace=True)\n",
    "            df_prcp_threshold.columns = ['end date', 'lon', 'lat', 'pr_sum']\n",
    "\n",
    "            df_prcp_threshold.insert(0, 'WCID', wcid)\n",
    "            df_prcp_threshold.insert(0, 'Member', member_id)\n",
    "            df_prcp_threshold.insert(0, 'Projection_slice_ID', PROJECTION_ID)\n",
    "\n",
    "            save(filename, df_prcp_threshold)\n",
    "            df_prcp_threshold = None\n",
    "\n",
    "\n",
    "def str_to_cftime360(time_str: str) -> cftime:\n",
    "    \"\"\" \n",
    "    Apply string to cftime360 conversion to each item in an iterable \n",
    "    Turn '2024-09-15 00:30' into '2024-09-15-00-30' and then split by '-'\n",
    "    Final result: ['2024', '09', '15', '00', '30']\n",
    "    \"\"\"\n",
    "    year, month, day, hour, minute = map(int, time_str.replace(\":\", \"-\").replace(\" \", \"-\").split(\"-\"))\n",
    "    return cftime.Datetime360Day(year, month, day, hour, minute)\n",
    "\n",
    "\n",
    "def get_bin_counts(df_prcp: pd.DataFrame, \n",
    "                   year: int, \n",
    "                   month: int, \n",
    "                   member_id: int, \n",
    "                   duration: int, \n",
    "                   wcid):\n",
    "    \"\"\"\n",
    "    Calculate rainfall counts for specified bins relevant to the chosen event duration (e.g. 1-h, 3-h, 6-h)\n",
    "    For September, the first and second halves are counted separately for a reason that Kay explained to be (but I've forgotten)\n",
    "    \"\"\"\n",
    "    bins = BINS[duration]\n",
    "    filename = os.path.join(OUTPUT_PATH, f\"Rainfall_bin_counts_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Bin counts']\n",
    "\n",
    "    total_count = []\n",
    "\n",
    "    if month == 9:\n",
    "        df_prcp_copy = df_prcp\n",
    "        df_prcp_copy.insert(0, 'Time', df_prcp_copy.index)\n",
    "        df_prcp_copy['Time'] = df_prcp_copy['Time'].apply(str_to_cftime360)\n",
    "        mid_sept_date = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "        start_sept_date = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "        df_prcp_copy = df_prcp_copy[(df_prcp_copy['Time'] >= start_sept_date) & (df_prcp_copy['Time'] <= mid_sept_date)]\n",
    "        for i in range(1, len(bins)):\n",
    "            df_count = df_prcp_copy[(df_prcp_copy['pr_sum'] > bins[i - 1]) & (df_prcp_copy['pr_sum'] < bins[i])]\n",
    "            total_count.append(df_count.shape[0])\n",
    "        data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "        total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "        save(filename, total_count_df)\n",
    "\n",
    "        data_list = None\n",
    "        total_count = None\n",
    "        total_count_df = None\n",
    "    \n",
    "    else:\n",
    "        for i in range(1, len(bins)):\n",
    "            df_count = df_prcp[(df_prcp['pr_sum'] > bins[i - 1]) & (df_prcp['pr_sum'] < bins[i])]\n",
    "            total_count.append(df_count.shape[0])\n",
    "        data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "        total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "        save(filename, total_count_df)\n",
    "\n",
    "        data_list = None\n",
    "        total_count = None\n",
    "        total_count_df = None\n",
    "\n",
    "def get_dry_days(ds_precip: xr.Dataset,\n",
    "                 year: int, \n",
    "                 month: int, \n",
    "                 member_id: int, \n",
    "                 wcid: int):\n",
    "    \"\"\"\n",
    "    Dry day counts from UKCP18 daily precipitation data (xr.ds)\n",
    "    Calculate the number of dry days per month for each grid point, and then calculate the mean\n",
    "    Calls `save_dry_counts` to write data out to csv file \n",
    "    \"\"\"\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        ds_month = ds_precip.where(ds_precip['time'].dt.month == month, drop=True)\n",
    "        daily_precip = ds_month['pr'].resample(time=\"D\").sum()\n",
    "        da_dry_days = daily_precip.where(daily_precip < 0.1)\n",
    "        da_dry_day_count = da_dry_days.count(dim=\"time\")\n",
    "        save_dry_counts(da_dry_day_count, year, month, member_id, wcid)\n",
    "\n",
    "        if month == 9:\n",
    "            start_sept = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "            ds_sept = ds_precip.where((ds_precip['time'] <= start_sept) & (ds_precip['time'].dt.month == month), drop=True)\n",
    "            daily_precip = ds_sept['pr'].resample(time=\"D\").sum()\n",
    "            da_dry_days_sept = daily_precip.where(daily_precip < 0.1)\n",
    "            da_dry_day_count_sept = da_dry_days_sept.count(dim=\"time\")\n",
    "            save_dry_counts(da_dry_day_count_sept, year, 13, member_id, wcid)\n",
    "\n",
    "\n",
    "def get_month_total(ds: xr.Dataset, \n",
    "                    year: int, \n",
    "                    month: int, \n",
    "                    member_id: int, \n",
    "                    wcid: int):\n",
    "    \"\"\"\n",
    "    This function calculates total monthly precip\n",
    "    Calls `save_month_total' to write data out to csv file \n",
    "    \"\"\"\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        start = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "        ds_month = ds.where(ds['time'] >= start, drop=True)\n",
    "        da_month_sum = ds_month.sum(dim='time')\n",
    "        save_month_total(da_month_sum, year, month, member_id, wcid)\n",
    "\n",
    "        if month == 9:\n",
    "            start9 = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "            ds_month9 = ds_month.where(ds['time'] <= start9, drop=True)\n",
    "            da_sum = ds_month9.sum(dim='time')\n",
    "            save_month_total(da_sum, year, 13, member_id, wcid)\n",
    "\n",
    "\n",
    "def save_dry_counts(ds_dry_days: xr.Dataset, \n",
    "                    year: int, \n",
    "                    month: int, \n",
    "                    member_id: int, \n",
    "                    wcid: int):\n",
    "    \"\"\" \n",
    "    Calculate the mean number of dry days in a given month over all grid points \n",
    "    Save this dry day count data to a csv file \n",
    "    Called by `get_dry_days`\n",
    "    \"\"\"\n",
    "    filename = os.path.join(OUTPUT_PATH, f\"Dry_days_counts_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "\n",
    "    ds_mask = ds_dry_days.mean()\n",
    "    dry_list = [PROJECTION_ID, member_id, year, month, wcid, ds_mask.values.tolist()]\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Mean dry day counts']\n",
    "    df = pd.DataFrame([dry_list], columns=cols)\n",
    "    save(filename, df)\n",
    "\n",
    "\n",
    "def save_month_total(ds_month_sum: xr.Dataset, \n",
    "                     year: int, \n",
    "                     month: int, \n",
    "                     member_id: int, \n",
    "                     wcid: int):\n",
    "    \"\"\" \n",
    "    Save total monthly precip to a csv file \n",
    "    Called by `get_month_total` \n",
    "    \"\"\"\n",
    "    filename = os.path.join(OUTPUT_PATH, f\"Total_rainfall_ens{m}_proj{PROJECTION_ID}.csv\")\n",
    "    ds_mask = ds_month_sum.mean()\n",
    "    dry_list = [PROJECTION_ID, member_id, year, month, wcid, ds_mask.pr.values.tolist()]\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Mean total rainfall']\n",
    "    df = pd.DataFrame([dry_list], columns=cols)\n",
    "    save(filename, df)\n",
    "\n",
    "\n",
    "def get_file_name(year: int, \n",
    "                  month: int , \n",
    "                  ensemble_member: int\n",
    "                  ) -> str:\n",
    "    \"\"\" \n",
    "    Return string for UKCP file name specific to a month, year and ensemble member\n",
    "    \"\"\"\n",
    "    start_date = f\"{year:04d}{month:02d}01\"\n",
    "    url=f\"https://dap.ceda.ac.uk/badc/ukcp18/data/land-cpm/uk/2.2km/rcp85/{ensemble_member:02d}/pr/1hr/v20210615/\"\n",
    "    file_main = f\"pr_rcp85_land-cpm_uk_2.2km_{ensemble_member:02d}_1hr_{start_date}\"\n",
    "    file_name = os.path.join(url, file_main + f\"-{year:04d}{month:02d}30.nc\")\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def get_start_year(year: int, \n",
    "                   month: int, \n",
    "                   hour: int):\n",
    "    \"\"\" \n",
    "    This function provides a buffer around the selected date\n",
    "    Starts the analysis on the 30th of the previous month\n",
    "    (i.e. 30th June 1981 if the user chose July 1981)\n",
    "    \"\"\"\n",
    "    if year != 1980:\n",
    "        year1 = year #1981\n",
    "        month1 = month - 1 #6\n",
    "        if month == 1:\n",
    "            year1 = year - 1\n",
    "            month1 = 12\n",
    "        start = cftime.Datetime360Day(year1, month1, 30, hour, 0, 0)\n",
    "    else:\n",
    "        month = 12\n",
    "        start = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "\n",
    "    return start\n",
    "\n",
    "\n",
    "def call_main(proj_df: pd.DataFrame, \n",
    "              month: int, \n",
    "              year: int, \n",
    "              mem_id: int,\n",
    "              mask_1D: xr.Dataset\n",
    "              ):\n",
    "    \"\"\" \n",
    "    Call the main function to read in UKCP data for the chosen year and month \n",
    "    \"\"\"\n",
    "    global OUTPUT_PATH\n",
    "    global PROJECTION_ID\n",
    "    mem = [mem_id]\n",
    "\n",
    "    for m in mem:\n",
    "        OUTPUT_PATH = f\"{SAVE_PATH}/precip_profiles/proj{PROJECTION_ID}/output_mem{m}\"\n",
    "        if not os.path.isdir(OUTPUT_PATH):\n",
    "            os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "        df_row = proj_df[(proj_df['Month'] == month) & (proj_df['Year'] == year)]\n",
    "\n",
    "        if df_row.empty:\n",
    "            print(f\"No data found for Month: {month}, Year: {year}\")\n",
    "            continue  # Skip to the next iteration\n",
    "        \n",
    "        year = int(df_row['Year'].iloc[0])\n",
    "        month = int(df_row['Month'].iloc[0])\n",
    "\n",
    "        year1 = year\n",
    "        month1 = month - 1\n",
    "        if month == 1:\n",
    "            year1 = year - 1\n",
    "            month1 = 12\n",
    "\n",
    "        file_name = get_file_name(year, month, m)\n",
    "        pre_file_name = get_file_name(year1, month1, m)\n",
    "\n",
    "        if (year==1980 and month==12) or (year==2020 and month==12) or (year==2060 and month==12):\n",
    "            infile = [file_name]\n",
    "        else:\n",
    "            infile = [pre_file_name, file_name]\n",
    "        main(infile, year, month, m, mask_1D)\n",
    "\n",
    "\n",
    "def check_dir(file_name: str):\n",
    "    \"\"\" \n",
    "    check if a directory exists, and create one if not \n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(file_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save(file_name: str, \n",
    "         df: pd.DataFrame\n",
    "         ):\n",
    "    \"\"\" \n",
    "    save pandas dataframe as a csv \n",
    "    \"\"\"\n",
    "    check_dir(file_name)\n",
    "    if os.path.isfile(file_name):\n",
    "        df.to_csv(file_name, mode='a', header=False, index=False, float_format=\"%.2f\")\n",
    "    else:\n",
    "        df.to_csv(file_name, mode='a', index=False, float_format=\"%.2f\")\n",
    "\n",
    "\n",
    "def rolling_window_sum(ds: xr.Dataset, \n",
    "                       window_size: int\n",
    "                       ) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    rolling window calculation for an xr.ds by defined window size (1-h, 3-h, 6-h, etc)\n",
    "    \"\"\"\n",
    "    print(f\"Starting calculation of rolling {str(window_size)}-h accumulated precip!\")\n",
    "    ds_window = ds.rolling(time=window_size, min_periods=window_size).construct(\"new\").sum(\"new\", skipna=True)\n",
    "    print(f\"Finished calculating rolling {str(window_size)}-h accumulated precip!\")\n",
    "\n",
    "    return ds_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing token at /home/shardy08/.cedatoken, skipping authentication.\n",
      "Fetching information about variable 'pr':\n",
      "Using download token 'eyJhb...A_LBQ' for authentication. Token expires at: 2024-12-19 14:00:40.166194+00:00.\n",
      "Opening Dataset from file pr_rcp85_land-cpm_uk_2.2km_04_1hr_20350801-20350830.nc ...\n",
      "Opening Dataset from file pr_rcp85_land-cpm_uk_2.2km_04_1hr_20350901-20350930.nc ...\n",
      "Finished reading in data from Opendap!\n",
      "Working on water company 0\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "37448\n",
      "5231\n",
      "719\n",
      "48\n",
      "32\n",
      "9\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "71711\n",
      "15141\n",
      "2526\n",
      "700\n",
      "62\n",
      "10\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 1\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "26693\n",
      "2919\n",
      "738\n",
      "246\n",
      "103\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "55750\n",
      "8178\n",
      "1408\n",
      "457\n",
      "75\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 2\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "7690\n",
      "1024\n",
      "278\n",
      "85\n",
      "64\n",
      "10\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "17747\n",
      "2076\n",
      "330\n",
      "153\n",
      "49\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 3\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "6323\n",
      "671\n",
      "39\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "14694\n",
      "1341\n",
      "35\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 4\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "6016\n",
      "1061\n",
      "379\n",
      "107\n",
      "99\n",
      "23\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "13854\n",
      "1942\n",
      "472\n",
      "299\n",
      "98\n",
      "15\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 5\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "3269\n",
      "352\n",
      "41\n",
      "6\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "7315\n",
      "746\n",
      "38\n",
      "12\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 6\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "13295\n",
      "1825\n",
      "515\n",
      "191\n",
      "123\n",
      "35\n",
      "17\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "30544\n",
      "3877\n",
      "744\n",
      "343\n",
      "103\n",
      "26\n",
      "15\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 7\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "13245\n",
      "1001\n",
      "143\n",
      "13\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "27575\n",
      "2056\n",
      "174\n",
      "13\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 8\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "6434\n",
      "1795\n",
      "493\n",
      "152\n",
      "60\n",
      "10\n",
      "4\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "13240\n",
      "3320\n",
      "660\n",
      "256\n",
      "53\n",
      "7\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 9\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "6103\n",
      "1581\n",
      "583\n",
      "215\n",
      "157\n",
      "63\n",
      "16\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "11573\n",
      "3319\n",
      "904\n",
      "447\n",
      "219\n",
      "53\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 10\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "8801\n",
      "2497\n",
      "605\n",
      "85\n",
      "13\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "14057\n",
      "5312\n",
      "1233\n",
      "173\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 11\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "9013\n",
      "3657\n",
      "1786\n",
      "346\n",
      "147\n",
      "64\n",
      "23\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "14807\n",
      "6387\n",
      "2560\n",
      "1268\n",
      "200\n",
      "101\n",
      "17\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Working on water company 12\n",
      "Starting dry days calculation!\n",
      "Calculating 1-h accumulated precip, starting at 23Z\n",
      "Starting dry days calculation!\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n",
      "6674\n",
      "818\n",
      "113\n",
      "17\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Starting dry days calculation!\n",
      "Calculating 6-h accumulated precip, starting at 19Z\n",
      "Starting calculation of rolling 6-h accumulated precip!\n",
      "Finished calculating rolling 6-h accumulated precip!\n",
      "14947\n",
      "1539\n",
      "113\n",
      "23\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "This code took : 379.3507933000001  (s) to run...\n"
     ]
    }
   ],
   "source": [
    "def run_notebook_functions(INPUT_PATH: str, \n",
    "                           mask_nc_filename: str, \n",
    "                           projection_id: int, \n",
    "                           ensemble_member_id: int): \n",
    "    \"\"\" \n",
    "    Run specified functions to process UKCP18 daily precipitation data for a given date (month,year), ensemble member and time slice \n",
    "    \"\"\"\n",
    "    global PROJECTION_ID\n",
    "    PROJECTION_ID = projection_id\n",
    "    profile_selected_month = pd.read_csv(os.path.join(INPUT_PATH, \"YearsMonths_byBinCounts_Rand_OtherYears.csv\"))\n",
    "\n",
    "    mask_nc = os.path.join(INPUT_PATH, mask_nc_filename)\n",
    "    mask_orig = xr.open_dataset(mask_nc)\n",
    "    mask_1D = mask_orig.stack(location=(\"grid_latitude\", \"grid_longitude\"))\n",
    "\n",
    "    projection_profile = profile_selected_month[profile_selected_month['Projection_slice_ID']\n",
    "                                                == projection_id]\n",
    "    \n",
    "    call_main(projection_profile, month, year, ensemble_member_id, mask_1D)\n",
    "\n",
    "run_notebook_functions(INPUT_PATH, mask_nc_filename, projection_id, ensemble_member_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024s1475-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
