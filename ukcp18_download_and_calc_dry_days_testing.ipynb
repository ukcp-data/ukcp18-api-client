{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UKCP 18 rainfall processing test notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and local routines \n",
    "\n",
    "- import local routine: `convert_ll2str.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.config\n",
    "import cftime\n",
    "import timeit\n",
    "import convert_ll2str as c2str\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from base64 import b64encode\n",
    "from datetime import datetime, timezone\n",
    "from getpass import getpass\n",
    "from netCDF4 import Dataset\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "\n",
    "- Calculating 3-h rolling precipitation totals \n",
    "- Calculating data for 13 water companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2035\n",
    "month = 9\n",
    "ensemble_member_id = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = str(Path.home())\n",
    "\n",
    "mask_nc_filename = \"UKWC_Cleaned_land-cpm_uk_2.2km.nc\"\n",
    "csv_filename = \"YearsMonths_byBinCounts_Rand_OtherYears.csv\"\n",
    "projection_id = 2 # 2021-2040 time slice\n",
    "var_id = 'pr'\n",
    "\n",
    "BINS = {3: [2, 6, 10, 15, 20, 30, 40, 60, 80, 110, 140, 175, 215, 265]}\n",
    "\n",
    "# these values represent the rainfall thresholds for each interval (1-h, 3-h, 6-h) corresponding to different RPs \n",
    "RPS = {3: [29, 35, 44, 49, 57]}\n",
    "\n",
    "rp_years = [5, 10, 30, 50, 100]\n",
    "\n",
    "global OUTPUT_PATH\n",
    "global PROJECTION_ID\n",
    "\n",
    "INPUT_PATH = '/mnt/c/Users/samhardy/OneDrive - JBA Group/2024s1475_RED_UP/ukcp_data/'\n",
    "SAVE_PATH = '/mnt/c/Users/samhardy/OneDrive - JBA Group/2024s1475_RED_UP/'\n",
    "NUMBER_OF_WC = 1\n",
    "\n",
    "accum_duration_start = {3: 22}\n",
    "MASK = None\n",
    "\n",
    "remove_items = ['ensemble_member_id', 'grid_latitude_bnds', 'grid_longitude_bnds',\n",
    "                'time_bnds','rotated_latitude_longitude', 'year', 'yyyymmddhh', 'ensemble_member']\n",
    "squeeze_coords = [\"bnds\", \"ensemble_member\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for retrieving UKCP data using an API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "remote_nc_with_token.py\n",
    "===================\n",
    "\n",
    "Python script for reading a NetCDF file remotely from the CEDA archive. It demonstrates fetching\n",
    "and using a download token to authenticate access to CEDA Archive data, as well as how to load\n",
    "and subset the Dataset from a stream of data (diskless), without having to download the whole file.\n",
    "\n",
    "Pre-requisites:\n",
    "\n",
    " - Python3.x\n",
    " - Python libraries (installed by Pip):\n",
    "\n",
    "```\n",
    "netCDF4\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```\n",
    "$ python remote_nc_with_token.py <url> <var_id>\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "$ URL=https://dap.ceda.ac.uk/badc/ukcp18/data/marine-sim/skew-trend/rcp85/skewSurgeTrend/latest/skewSurgeTrend_marine-sim_rcp85_trend_2007-2099.nc\n",
    "$ VAR_ID=skewSurgeTrend\n",
    "\n",
    "$ python remote_nc_with_token.py $URL $VAR_ID\n",
    "```\n",
    "\n",
    "You will be prompted to provide your CEDA username and password the first time the script is run and\n",
    "again if the token cached from a previous attempt has expired.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# URL for the CEDA Token API service\n",
    "TOKEN_URL = \"https://services-beta.ceda.ac.uk/api/token/create/\"\n",
    "# Location on the filesystem to store a cached download token\n",
    "TOKEN_CACHE = os.path.expanduser(os.path.join(\"~\", \".cedatoken\"))\n",
    "\n",
    "\n",
    "def load_cached_token():\n",
    "    \"\"\"\n",
    "    Read the token back out from its cache file.\n",
    "\n",
    "    Returns a tuple containing the token and its expiry timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the token back out from its cache file\n",
    "    try:\n",
    "        with open(TOKEN_CACHE, \"r\") as cache_file:\n",
    "            data = json.loads(cache_file.read())\n",
    "\n",
    "            token = data.get(\"access_token\")\n",
    "            expires = datetime.strptime(data.get(\"expires\"), \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "            return token, expires\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_token():\n",
    "    \"\"\"\n",
    "    Fetches a download token, either from a cache file or\n",
    "    from the token API using CEDA login credentials.\n",
    "\n",
    "    Returns an active download token\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the cache file to see if we already have an active token\n",
    "    token, expires = load_cached_token()\n",
    "\n",
    "    # If no token has been cached or the token has expired, we get a new one\n",
    "    now = datetime.now(timezone.utc)\n",
    "    if not token or expires < now:\n",
    "\n",
    "        if not token:\n",
    "            print(f\"No previous token found at {TOKEN_CACHE}. \", end=\"\")\n",
    "        else:\n",
    "            print(f\"Token at {TOKEN_CACHE} has expired. \", end=\"\")\n",
    "        print(\"Generating a fresh token...\")\n",
    "\n",
    "        print(\"Please provide your CEDA username: \", end=\"\")\n",
    "        username = input()\n",
    "        password = getpass(prompt=\"CEDA user password: \")\n",
    "\n",
    "        credentials = b64encode(f\"{username}:{password}\".encode(\"utf-8\")).decode(\n",
    "            \"ascii\"\n",
    "        )\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Basic {credentials}\",\n",
    "        }\n",
    "        response = requests.request(\"POST\", TOKEN_URL, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # The token endpoint returns JSON\n",
    "            response_data = json.loads(response.text)\n",
    "            token = response_data[\"access_token\"]\n",
    "\n",
    "            # Store the JSON data in the cache file for future use\n",
    "            with open(TOKEN_CACHE, \"w\") as cache_file:\n",
    "                cache_file.write(response.text)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to generate token, check your username and password.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Found existing token at {TOKEN_CACHE}, skipping authentication.\")\n",
    "\n",
    "    return token, expires\n",
    "\n",
    "\n",
    "def open_datasets(urls: list[str],\n",
    "                  download_token=None\n",
    "                  ):\n",
    "    \"\"\" \n",
    "    Open a list of NetCDF datasets from specified URLs. \n",
    "    \"\"\"\n",
    "\n",
    "    datasets = []\n",
    "    headers = None\n",
    "\n",
    "    if download_token:\n",
    "        headers = {\"Authorization\": f\"Bearer {download_token}\"}\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.request(\"GET\", url, headers=headers, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            print(\n",
    "                f\"Failed to fetch data. The response from the server was {response.status_code}\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        print(f\"Opening Dataset from file {filename} ...\")\n",
    "        datasets.append(Dataset(filename, memory=response.content))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def initiate_opendap_multiple_files(urls: list[str], \n",
    "                                    var_id: str\n",
    "                                    ) -> xr.Dataset:\n",
    "    \"\"\" \n",
    "    Initiate an API call to download UKCP18 data for multiple year-month selections \n",
    "\n",
    "    Returns an xarray.dataset, concatenated if necessary and chunked using dask to reduce memory usage\n",
    "    \"\"\"\n",
    "    token, expires = get_token()\n",
    "    if token:\n",
    "        # Now that we have a valid token, we can attempt to open the Dataset from a URL.\n",
    "        # This will only work if the token is associated with a CEDA user that has been granted\n",
    "        # access to the data (i.e. if they can already download the file in a browser).\n",
    "        # \n",
    "        print(f\"Fetching information about variable '{var_id}':\")\n",
    "        if token:\n",
    "            print((\n",
    "                f\"Using download token '{token[:5]}...{token[-5:]}' for authentication.\"\n",
    "                f\" Token expires at: {expires}.\"\n",
    "            ))\n",
    "        else:\n",
    "            print(\"No DOWNLOAD_TOKEN found in environment.\")\n",
    "\n",
    "        nc_datasets = open_datasets(urls, download_token=token)\n",
    "\n",
    "        xarray_datasets = []\n",
    "        for nc_data in nc_datasets:\n",
    "            if nc_data is None:\n",
    "                continue\n",
    "\n",
    "            ds = xr.open_dataset(xr.backends.NetCDF4DataStore(nc_data), chunks={\"time\": 30})\n",
    "            xarray_datasets.append(ds)\n",
    "\n",
    "        # combine datasets using xarray if required \n",
    "        if len(xarray_datasets) > 1:\n",
    "            combined_ds = xr.concat(xarray_datasets, dim=\"time\").chunk({\"time\": 30})\n",
    "            return combined_ds\n",
    "        elif xarray_datasets:\n",
    "            return xarray_datasets[0]\n",
    "        else:\n",
    "            print(\"No datasets were opened!\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dry_days(da_precip: xr.Dataset,\n",
    "                 year: int, \n",
    "                 month: int, \n",
    "                 member_id: int, \n",
    "                 wcid: int):\n",
    "    \"\"\"\n",
    "    Dry day counts from UKCP18 daily precipitation data (xr.ds)\n",
    "    Calls `save_dry_counts` to write data out to csv file \n",
    "    \"\"\"\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        da_day_dry = da_precip.where(da_precip < 0.1)\n",
    "        da_dry_count = da_day_dry.count(dim=\"time\")\n",
    "        save_dry_counts(da_dry_count, year, month, member_id, wcid)\n",
    "\n",
    "        if month == 9:\n",
    "            start9 = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "            ds_month9 = da_precip.where(da_precip['time'] <= start9, drop=True)\n",
    "            da_day_dry9 = ds_month9.where(ds_month9 < 0.1)\n",
    "            da_dry_count9 = da_day_dry9.count(dim=\"time\")\n",
    "            save_dry_counts(da_dry_count9, year, 13, member_id, wcid)\n",
    "\n",
    "\n",
    "def get_bin_counts(df_prcp: pd.DataFrame, \n",
    "                   year: int, \n",
    "                   month: int, \n",
    "                   member_id: int, \n",
    "                   duration: int, \n",
    "                   wcid):\n",
    "    \"\"\"\n",
    "    Calculate rainfall counts for specified bins relevant to the chosen event duration (e.g. 1-h, 3-h, 6-h)\n",
    "    \"\"\"\n",
    "    bins = BINS[duration]\n",
    "    filename = os.path.join(OUTPUT_PATH, f\"Rainfall_bin_counts_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Bin counts']\n",
    "\n",
    "    total_count = []\n",
    "\n",
    "    for i in range(1, len(bins)):\n",
    "        df_count = df_prcp[(df_prcp['pr_sum'] > bins[i - 1]) & (df_prcp['pr_sum'] < bins[i])]\n",
    "        total_count.append(df_count.shape[0])\n",
    "    data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "    total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "    save(filename, total_count_df)\n",
    "\n",
    "    data_list = None\n",
    "    total_count = None\n",
    "    total_count_df = None\n",
    "\n",
    "    if month == 9:\n",
    "        df_prcp_copy = df_prcp\n",
    "        df_prcp_copy.insert(0, 'Time', df_prcp_copy.index)\n",
    "        start_sept_date = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "        df_prcp_copy = df_prcp_copy[df_prcp_copy['Time'] <= start_sept_date]\n",
    "        for i in range(1, len(bins)):\n",
    "            df_count = df_prcp_copy[(df_prcp_copy['pr_sum'] > bins[i - 1]) & (df_prcp_copy['pr_sum'] < bins[i])]\n",
    "            total_count.append(df_count.shape[0])\n",
    "        data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "        total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "        save(filename, total_count_df)\n",
    "\n",
    "        data_list = None\n",
    "        total_count = None\n",
    "        total_count_df = None\n",
    "\n",
    "\n",
    "def save_dry_counts(ds_dry_days: xr.Dataset, \n",
    "                    year: int, \n",
    "                    month: int, \n",
    "                    member_id: int, \n",
    "                    wcid: int):\n",
    "    \"\"\" \n",
    "    Save dry day count data to a csv file \n",
    "    Called by `get_dry_days`\n",
    "    \"\"\"\n",
    "    filename = os.path.join(\"./\", f\"Dry_days_counts_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "\n",
    "    ds_mask = ds_dry_days.mean()\n",
    "    dry_list = [PROJECTION_ID, member_id, year, month, wcid, ds_mask.pr.values.tolist()]\n",
    "    cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Mean dry day counts']\n",
    "    df = pd.DataFrame([dry_list], columns=cols)\n",
    "    save(filename, df)\n",
    "\n",
    "\n",
    "def get_file_name(year: int, \n",
    "                  month: int , \n",
    "                  ensemble_member: int\n",
    "                  ) -> str:\n",
    "    \"\"\" \n",
    "    Return string for UKCP file name specific to a month, year and ensemble member\n",
    "    \"\"\"\n",
    "    start_date = f\"{year:04d}{month:02d}01\"\n",
    "    url=f\"https://dap.ceda.ac.uk/badc/ukcp18/data/land-cpm/uk/2.2km/rcp85/{ensemble_member:02d}/pr/1hr/v20210615/\"\n",
    "    file_main = f\"pr_rcp85_land-cpm_uk_2.2km_{ensemble_member:02d}_1hr_{start_date}\"\n",
    "    file_name = os.path.join(url, file_main + f\"-{year:04d}{month:02d}30.nc\")\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def get_start_year(year: int, \n",
    "                   month: int, \n",
    "                   hour: int):\n",
    "    \"\"\" \n",
    "    This function provides a buffer around the selected date\n",
    "    Starts the analysis on the 30th of the previous month\n",
    "    (i.e. 30th June 1981 if the user chose July 1981)\n",
    "    \"\"\"\n",
    "    if year != 1980:\n",
    "        year1 = year #1981\n",
    "        month1 = month - 1 #6\n",
    "        if month == 1:\n",
    "            year1 = year - 1\n",
    "            month1 = 12\n",
    "        start = cftime.Datetime360Day(year1, month1, 30, hour, 0, 0)\n",
    "    else:\n",
    "        month = 12\n",
    "        start = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "\n",
    "    return start\n",
    "\n",
    "\n",
    "def check_dir(file_name: str):\n",
    "    \"\"\" \n",
    "    check if a directory exists, and create one if not \n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(file_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save(file_name: str, \n",
    "         df: pd.DataFrame\n",
    "         ):\n",
    "    \"\"\" \n",
    "    save pandas dataframe as a csv \n",
    "    \"\"\"\n",
    "    check_dir(file_name)\n",
    "    if os.path.isfile(file_name):\n",
    "        df.to_csv(file_name, mode='a', header=False, index=False, float_format=\"%.2f\")\n",
    "    else:\n",
    "        df.to_csv(file_name, mode='a', index=False, float_format=\"%.2f\")\n",
    "\n",
    "\n",
    "def rolling_window_sum(ds: xr.Dataset, \n",
    "                       window_size: int\n",
    "                       ) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    rolling window calculation for an xr.ds by defined window size (1-h, 3-h, 6-h, etc)\n",
    "    \"\"\"\n",
    "    print(f\"Starting calculation of rolling {str(window_size)}-h accumulated precip!\")\n",
    "    ds_window = ds.rolling(time=window_size, min_periods=window_size).construct(\"new\").sum(\"new\", skipna=True)\n",
    "    print(f\"Finished calculating rolling {str(window_size)}-h accumulated precip!\")\n",
    "\n",
    "    return ds_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing token at /home/shardy08/.cedatoken, skipping authentication.\n",
      "Fetching information about variable 'pr':\n",
      "Using download token 'eyJhb...A_LBQ' for authentication. Token expires at: 2024-12-19 14:00:40.166194+00:00.\n",
      "Opening Dataset from file pr_rcp85_land-cpm_uk_2.2km_04_1hr_20350801-20350830.nc ...\n",
      "Opening Dataset from file pr_rcp85_land-cpm_uk_2.2km_04_1hr_20350901-20350930.nc ...\n",
      "Finished reading in data from Opendap!\n",
      "Working on water company 0\n",
      "Calculating 3-h accumulated precip, starting at 22Z\n",
      "Starting calculation of rolling 3-h accumulated precip!\n",
      "Finished calculating rolling 3-h accumulated precip!\n"
     ]
    }
   ],
   "source": [
    "year = 2035\n",
    "month = 9\n",
    "member_id = 4\n",
    "\n",
    "def run_notebook_functions(INPUT_PATH: str, \n",
    "                           mask_nc_filename: str, \n",
    "                           projection_id: int, \n",
    "                           member_id: int,\n",
    "                           month: int,\n",
    "                           year: int): \n",
    "    \"\"\" \n",
    "    Run specified functions to process UKCP18 daily precipitation data for a given date (month,year), ensemble member and time slice \n",
    "    \"\"\"\n",
    "    global PROJECTION_ID\n",
    "    PROJECTION_ID = projection_id\n",
    "    profile_selected_month = pd.read_csv(os.path.join(INPUT_PATH, \"YearsMonths_byBinCounts_Rand_OtherYears.csv\"))\n",
    "\n",
    "    mask_nc = os.path.join(INPUT_PATH, mask_nc_filename)\n",
    "    mask_orig = xr.open_dataset(mask_nc)\n",
    "    mask_1D = mask_orig.stack(location=(\"grid_latitude\", \"grid_longitude\"))\n",
    "\n",
    "    projection_profile = profile_selected_month[profile_selected_month['Projection_slice_ID']\n",
    "                                                == projection_id]\n",
    "    \n",
    "    # call_main function \n",
    "    df_row = projection_profile[(projection_profile['Month'] == month) & (projection_profile['Year'] == year)]\n",
    "    if df_row.empty:\n",
    "        print(f\"No data found for Month: {month}, Year: {year}\")\n",
    "    \n",
    "    year = int(df_row['Year'].iloc[0])\n",
    "    month = int(df_row['Month'].iloc[0])\n",
    "\n",
    "    year1 = year\n",
    "    month1 = month - 1\n",
    "    if month == 1:\n",
    "        year1 = year - 1\n",
    "        month1 = 12\n",
    "\n",
    "    file_name = get_file_name(year, month, member_id)\n",
    "    pre_file_name = get_file_name(year1, month1, member_id)\n",
    "\n",
    "    if (year==1980 and month==12) or (year==2020 and month==12) or (year==2060 and month==12):\n",
    "        infile = [file_name]\n",
    "    else:\n",
    "        infile = [pre_file_name, file_name]\n",
    "\n",
    "    with initiate_opendap_multiple_files(infile,var_id) as ds:\n",
    "        print(\"Finished reading in data from Opendap!\")\n",
    "\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "            ds = ds.stack(location=(\"grid_latitude\", \"grid_longitude\"))\n",
    "            ids = c2str.get_cell_ids(ds.location.values)\n",
    "            ds.coords['location_id'] = ('location', ids)\n",
    "            ds = ds.where(ds.bnds == 0, drop=True)\n",
    "            for item in remove_items:\n",
    "                del ds[item]\n",
    "            for item in squeeze_coords:\n",
    "                ds = ds.squeeze(item)\n",
    "\n",
    "            ds = ds.where(mask_1D[\"WCID\"] >= 0, drop=True)\n",
    "\n",
    "            starttime = timeit.default_timer()\n",
    "\n",
    "            print(f\"Working on water company {str(0)}\")\n",
    "            ds_mask = ds.where(mask_1D.WCID == 0, drop=True)\n",
    "            duration = 3; start_hour = 22\n",
    "            start = get_start_year(year, month, start_hour)\n",
    "            precip = ds_mask.where(ds['time'] >= start, drop=True)\n",
    "            print(f\"Calculating {str(duration)}-h accumulated precip, starting at {str(start_hour)}Z\")\n",
    "\n",
    "            ds_window = rolling_window_sum(precip, duration)\n",
    "            ds_window = ds_window.rename({\"pr\": \"pr_sum\"})\n",
    "            ds_window = ds_window.assign(pr=precip.pr)\n",
    "            ds_window['time'] = ds_window[\"time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "            df_window = ds_window.to_dataframe()\n",
    "            df_window.index = df_window.index.droplevel(['grid_latitude', 'grid_longitude'])\n",
    "            \n",
    "    return precip, df_window\n",
    "\n",
    "ds_precip, df_precip = run_notebook_functions(INPUT_PATH, mask_nc_filename, projection_id, member_id, month, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_bin_counts` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_cftime360(time_str: str) -> cftime:\n",
    "    \"\"\" \n",
    "    Apply string to cftime360 conversion to each item in an iterable \n",
    "    Turn '2024-09-15 00:30' into '2024-09-15-00-30' and then split by '-'\n",
    "    Final result: ['2024', '09', '15', '00', '30']\n",
    "    \"\"\"\n",
    "    year, month, day, hour, minute = map(int, time_str.replace(\":\", \"-\").replace(\" \", \"-\").split(\"-\"))\n",
    "    return cftime.Datetime360Day(year, month, day, hour, minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prcp_copy = df_precip\n",
    "df_prcp_copy.insert(0, 'Time', df_prcp_copy.index)\n",
    "df_prcp_copy['Time'] = df_prcp_copy['Time'].apply(str_to_cftime360)\n",
    "mid_sept_date = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "start_sept_date = cftime.Datetime360Day(year, month, 1, 0, 30, 0)\n",
    "df_prcp_copy = df_prcp_copy[(df_prcp_copy['Time'] >= start_sept_date) & (df_prcp_copy['Time'] <= mid_sept_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>grid_latitude</th>\n",
       "      <th>grid_longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>month_number</th>\n",
       "      <th>location_id</th>\n",
       "      <th>pr_sum</th>\n",
       "      <th>pr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2035-09-01 00:30</th>\n",
       "      <td>2035-09-01 00:30:00</td>\n",
       "      <td>2.840050</td>\n",
       "      <td>358.130737</td>\n",
       "      <td>55.297592</td>\n",
       "      <td>-5.780537</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00284N_35813E</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>5.973412e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-01 00:30</th>\n",
       "      <td>2035-09-01 00:30:00</td>\n",
       "      <td>2.840050</td>\n",
       "      <td>358.150757</td>\n",
       "      <td>55.298496</td>\n",
       "      <td>-5.745451</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00284N_35815E</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>6.582984e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-01 00:30</th>\n",
       "      <td>2035-09-01 00:30:00</td>\n",
       "      <td>2.840050</td>\n",
       "      <td>358.170746</td>\n",
       "      <td>55.299389</td>\n",
       "      <td>-5.710418</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00284N_35817E</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>5.760635e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-01 00:30</th>\n",
       "      <td>2035-09-01 00:30:00</td>\n",
       "      <td>2.860050</td>\n",
       "      <td>358.130737</td>\n",
       "      <td>55.317571</td>\n",
       "      <td>-5.782135</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00286N_35813E</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.315604e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-01 00:30</th>\n",
       "      <td>2035-09-01 00:30:00</td>\n",
       "      <td>2.860050</td>\n",
       "      <td>358.150757</td>\n",
       "      <td>55.318475</td>\n",
       "      <td>-5.747032</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00286N_35815E</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>3.553440e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-15 00:30</th>\n",
       "      <td>2035-09-15 00:30:00</td>\n",
       "      <td>8.062849</td>\n",
       "      <td>360.650757</td>\n",
       "      <td>60.556943</td>\n",
       "      <td>-1.189136</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00806N_36065E</td>\n",
       "      <td>0.168826</td>\n",
       "      <td>1.276793e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-15 00:30</th>\n",
       "      <td>2035-09-15 00:30:00</td>\n",
       "      <td>8.062849</td>\n",
       "      <td>360.670746</td>\n",
       "      <td>60.556575</td>\n",
       "      <td>-1.148880</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00806N_36067E</td>\n",
       "      <td>1.098274</td>\n",
       "      <td>8.586697e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-15 00:30</th>\n",
       "      <td>2035-09-15 00:30:00</td>\n",
       "      <td>8.062849</td>\n",
       "      <td>360.690735</td>\n",
       "      <td>60.556195</td>\n",
       "      <td>-1.108626</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00806N_36069E</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.027349e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-15 00:30</th>\n",
       "      <td>2035-09-15 00:30:00</td>\n",
       "      <td>8.062849</td>\n",
       "      <td>360.710754</td>\n",
       "      <td>60.555804</td>\n",
       "      <td>-1.068311</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00806N_36071E</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2.433108e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035-09-15 00:30</th>\n",
       "      <td>2035-09-15 00:30:00</td>\n",
       "      <td>8.062849</td>\n",
       "      <td>360.730743</td>\n",
       "      <td>60.555402</td>\n",
       "      <td>-1.028058</td>\n",
       "      <td>9</td>\n",
       "      <td>UK_00806N_36073E</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>8.013113e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2557156 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Time  grid_latitude  grid_longitude  \\\n",
       "time                                                                   \n",
       "2035-09-01 00:30  2035-09-01 00:30:00       2.840050      358.130737   \n",
       "2035-09-01 00:30  2035-09-01 00:30:00       2.840050      358.150757   \n",
       "2035-09-01 00:30  2035-09-01 00:30:00       2.840050      358.170746   \n",
       "2035-09-01 00:30  2035-09-01 00:30:00       2.860050      358.130737   \n",
       "2035-09-01 00:30  2035-09-01 00:30:00       2.860050      358.150757   \n",
       "...                               ...            ...             ...   \n",
       "2035-09-15 00:30  2035-09-15 00:30:00       8.062849      360.650757   \n",
       "2035-09-15 00:30  2035-09-15 00:30:00       8.062849      360.670746   \n",
       "2035-09-15 00:30  2035-09-15 00:30:00       8.062849      360.690735   \n",
       "2035-09-15 00:30  2035-09-15 00:30:00       8.062849      360.710754   \n",
       "2035-09-15 00:30  2035-09-15 00:30:00       8.062849      360.730743   \n",
       "\n",
       "                   latitude  longitude  month_number       location_id  \\\n",
       "time                                                                     \n",
       "2035-09-01 00:30  55.297592  -5.780537             9  UK_00284N_35813E   \n",
       "2035-09-01 00:30  55.298496  -5.745451             9  UK_00284N_35815E   \n",
       "2035-09-01 00:30  55.299389  -5.710418             9  UK_00284N_35817E   \n",
       "2035-09-01 00:30  55.317571  -5.782135             9  UK_00286N_35813E   \n",
       "2035-09-01 00:30  55.318475  -5.747032             9  UK_00286N_35815E   \n",
       "...                     ...        ...           ...               ...   \n",
       "2035-09-15 00:30  60.556943  -1.189136             9  UK_00806N_36065E   \n",
       "2035-09-15 00:30  60.556575  -1.148880             9  UK_00806N_36067E   \n",
       "2035-09-15 00:30  60.556195  -1.108626             9  UK_00806N_36069E   \n",
       "2035-09-15 00:30  60.555804  -1.068311             9  UK_00806N_36071E   \n",
       "2035-09-15 00:30  60.555402  -1.028058             9  UK_00806N_36073E   \n",
       "\n",
       "                    pr_sum            pr  \n",
       "time                                      \n",
       "2035-09-01 00:30  0.000012  5.973412e-06  \n",
       "2035-09-01 00:30  0.000679  6.582984e-04  \n",
       "2035-09-01 00:30  0.005860  5.760635e-03  \n",
       "2035-09-01 00:30  0.000010  3.315604e-06  \n",
       "2035-09-01 00:30  0.000369  3.553440e-04  \n",
       "...                    ...           ...  \n",
       "2035-09-15 00:30  0.168826  1.276793e-01  \n",
       "2035-09-15 00:30  1.098274  8.586697e-01  \n",
       "2035-09-15 00:30  0.000004  1.027349e-06  \n",
       "2035-09-15 00:30  0.000026  2.433108e-05  \n",
       "2035-09-15 00:30  0.000002  8.013113e-07  \n",
       "\n",
       "[2557156 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_prcp_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration = 3\n",
    "# wcid = 0\n",
    "# bins = BINS[duration]\n",
    "# filename = os.path.join(\"./\", f\"Rainfall_bin_counts_{duration}h_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "# cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Bin counts']\n",
    "# total_count = []\n",
    "\n",
    "# for i in range(1, len(bins)):\n",
    "#     df_count = df_precip[(df_precip['pr_sum'] > bins[i - 1]) & (df_precip['pr_sum'] < bins[i])]\n",
    "#     total_count.append(df_count.shape[0])\n",
    "# data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "# total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "# save(filename, total_count_df)\n",
    "\n",
    "# data_list = None\n",
    "# total_count = None\n",
    "# total_count_df = None\n",
    "\n",
    "# if month == 9:\n",
    "#     df_prcp_copy = df_precip\n",
    "#     df_prcp_copy.insert(0, 'Time', df_prcp_copy.index)\n",
    "#     start_sept_date = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "#     df_prcp_copy = df_prcp_copy[df_prcp_copy['Time'] <= start_sept_date]\n",
    "#     for i in range(1, len(bins)):\n",
    "#         df_count = df_prcp_copy[(df_prcp_copy['pr_sum'] > bins[i - 1]) & (df_prcp_copy['pr_sum'] < bins[i])]\n",
    "#         total_count.append(df_count.shape[0])\n",
    "#     data_list = [PROJECTION_ID, member_id, year, month, wcid, total_count]\n",
    "#     total_count_df = pd.DataFrame([data_list], columns=cols)\n",
    "#     save(filename, total_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_dry_days` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_month = ds_precip.where(ds_precip['time'].dt.month == month, drop=True)\n",
    "# daily_precip = ds_precip['pr'].resample(time=\"D\").sum()\n",
    "# da_dry_days = daily_precip.where(daily_precip < 0.1)\n",
    "# da_dry_day_count = da_dry_days.count(dim=\"time\")\n",
    "\n",
    "# if month == 9:\n",
    "#     start_sept = cftime.Datetime360Day(year, month, 15, 0, 30, 0)\n",
    "#     ds_sept = ds_precip.where((ds_precip['time'] <= start_sept) & (ds_precip['time'].dt.month == month), drop=True)\n",
    "#     daily_precip = ds_sept['pr'].resample(time=\"D\").sum()\n",
    "#     da_dry_days_sept = daily_precip.where(daily_precip < 0.1)\n",
    "#     da_dry_day_count_sept = da_dry_days_sept.count(dim=\"time\")\n",
    "#     #save_dry_counts(da_dry_day_count_sept, year, 13, member_id, 0)\n",
    "\n",
    "# filename = os.path.join(\"./\", f\"Dry_days_counts_ens{member_id}_proj{PROJECTION_ID}.csv\")\n",
    "\n",
    "# ds_mask = da_dry_day_count_sept.mean()\n",
    "# dry_list = [PROJECTION_ID, member_id, year, month, 0, ds_mask.values.tolist()]\n",
    "# cols = ['Projection_slice_ID', 'Member', 'Year', 'Month', 'WCID', 'Mean dry day counts']\n",
    "# df = pd.DataFrame([dry_list], columns=cols)\n",
    "# save(filename, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024s1475-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
